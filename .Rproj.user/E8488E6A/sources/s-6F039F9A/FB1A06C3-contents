---
title: "BIO-MATE: A biological ocean data reformatting effort"
author: 
  name: |
    [Kimberlee Baldry](kimberlee.baldry@utas.edu.au) <a href="https://orcid.org/0000-0003-3286-8624"><img alt="ORCID logo" src="https://info.orcid.org/wp-content/uploads/2019/11/orcid_16x16.png" width="16" height="16" /></a> $^{*,a}$
    Robert Johnson <a href ="https://orcid.org/0000-0002-5915-5416"><img alt="ORCID logo" src="https://info.orcid.org/wp-content/uploads/2019/11/orcid_16x16.png" width="16" height="16" /></a> $^{b}$
    Peter G. Strutton <a href="https://orcid.org/0000-0002-2395-9471"><img alt="ORCID logo" src="https://info.orcid.org/wp-content/uploads/2019/11/orcid_16x16.png" width="16" height="16" /><a/> $^{a}$
    Philip W. Boyd <a href="https://orcid.org/0000-0001-7850-1911"><img alt="ORCID logo" src="https://info.orcid.org/wp-content/uploads/2019/11/orcid_16x16.png" width="16" height="16" /><a/> $^{a}$
  
  affiliation: |
    $*$: Corresponding author. E-mail: kimberlee.baldry@utas.edu.au
    $a$: Institute for Marine and Antarctic Studies, University of Tasmania, Hobart, Australia
    $b$: Bureau National Operations Centre, Bureau of Meteorology, Hobart, Australia

abstract: | 
  Biological ocean data collected from ships find reuse within aggregations of historical data. These data are heavily relied upon to document long term change, validate satellite algorithms for ocean biology and are useful in assessing the performance of autonomous platforms and bio-physical models. 
  There is a need to combine subsurface biological and physical data into one aggregate data product to support reproducible research. This is motivated by an era of increased use of biogeochemical ocean models and autonomous observations. Existing aggregate products are dissimilar in source data, have largely been isolated to the surface ocean and most omit physical data. These products cannot easily be used to explore subsurface bio-physical relationships.
  We present the first version of a biological ocean data reformatting effort (BIO-MATE). BIO-MATE is R software that reformats and aggregates openly sourced published datasets from oceanographic voyages (https://gitlab.com/KBaldry/BIO-MATE). Biological and physical data from underway sensors, profiling sensors, pigments analysis and particulate organic carbon analysis are stored in BIO-MATE.

bibliography: ["descriptor.bib"]
csl: nature.csl
output: html_document
---

```{r}
library(kableExtra)
library(tidyverse)
```

# Background & Summary
Marine phytoplankton blooms support ocean food-webs and influence global climate through the biological carbon pump (@schmitt2018; @ainley1991: @basu2018). Ocean physics and other environmental drivers control the timing, magnitude and extent of phytoplankton blooms through complex bio-physical relationships (@carranza2018; @prairie2012; @wihsgott2019; @brody2015). To study these bio-physical relationships integrated data structures that link biological and physical ocean data are needed. Ship-based data is the gold-standard for accurate biological oceanographic measurements (@mignot2019). These data are often published separately to physical ocean data, stored across different repositories and in multiple formats. This makes it difficult and time-consuming to aggregate and link biological and physical data. The described data product attempts to make this task easier.

The biological ocean data reformatting effort (BIO-MATE) works to link existing, open-access biological and physical datasets across oceanographic voyages and promote their re-use. This has been done by developing R based software that not only reformats published datasets, but also cross-references between biological and physical data (https://github.com/KimBaldry/BIOMATE-Rpackage). A final aggregate data product allows users to easily access, manipulate and cite published ship-based datasets of different dimensions for multiple applications. 

The BIO-MATE aggregate data product can be accessed via the Australian Ocean Data Network (https://portal.aodn.org.au/). The aggregation includes data collected from shipboard underway sensors, profiling sensors mounted on sampling rosettes, lab analysis for phytoplankton pigments and lab analysis for particulate organic carbon (POC). These data are stored as four data streams, cross-referenced by unique expedition codes (EXPOCODE) and profiling station identifications (CTD_ID). An additional data stream contains metadata for the data product including a list of oceanographic voyages, investigator contact information and data citations for reformatted datasets. We have also included an aggregated data table for biological data. Users are required to refer to metadata and cite all data products accessed through BIO-MATE, as well as the BIO-MATE data product itself. We consulted the distribution licenses of all data sources to ensure that with this condition data is re-used lawfully.

In the future BIO-MATE can be extended to include new cruises, new biological variables and an online interface to facilitate data submissions. The product is formatted to facilitate a global database and for the addition of new biological parameters (e.g. microscopic abundances) in future versions. We initially restricted this version of BIO-MATE to the Southern Ocean as it is historically underrepresented in previous aggregations of biological data: @valente2019, @sauzede2015, @davies2018, @uitz2006, MAREDAT (@peloquin2013) and NoMAD/SeaBASS (@SEABASS_1; @SEABASS_2; https://seabass.gsfc.nasa.gov/). This has led to a lack of understanding (@uitz2006) and poor performance of models due to the unique biogeochemistry of the vast region (@sauzede2015; @johnson2013).

To date, the data product has been used to address discrepancies in Southern Ocean satellite composites (Johnson et al. in prep), to understand how the response of in-situ fluorometers changes in the Southern Ocean (Baldry et al. in prep), and to investigate the role of ocean physics in mediating subsurface chlorophyll features (Baldry et al. in prep). These examples highlight the malleability of this data product to improve our understanding of biological oceanography in the Southern Ocean. Further uses include validating satellite observations (@valente2019; @johnson2013), developing new ways to validate in-situ bio-optical observations collected by autonomous profiling platforms in the presence of dynamic fronts (@sauzede2015b; @roesler2017; @mignot2019), training ocean state estimations (@verdy2017), informing bio-physical models () and using multi-variate analyses to understand bio-physical relationships ().

We recognise the massive effort in producing the thousands of data records within this data product. This includes the investigators and data officers who have spent countless hours in ship-time, project organisation, grant writing, laboratory analysis, post-processing data and report writing. Oceanographic data are often collected with regional studies in mind, but their value increases with publication and re-use. We encourage all investigators to publish their data for re-use through data products like BIO-MATE.

# Methods

#### *Published datasets in BIO-MATE* 
The BIO-MATE aggregate data product brings together ship-based data that has been collected by a Principal Investigator (PI) and openly published by a to a publically accessible data reprository /@ref(fig:method). The first version of BIO-MATE includes published dataset associated with four types of sampliong methods:
1. sensors in the vessels underway in-take seawater (underway sensor data stream), 
2. profiling sensors mounted to sampling rosettes (profiling sensor data stream),
3. pigments measured in the laboratory (pigment data stream),
4. and POC measured in the laboratory (POC data stream).

Data records from the pigment data stream were first identified within data reprositories that host biological data (November 2019). Pigment datasets were identified using the search term “chlorophyll” and a latitude bound of 30 $^\circ$S to 90 $^\circ$S from PANGAEA (https://www.pangaea.de/), SeaBASS (https://seabass.gsfc.nasa.gov/), the Australian Ocean Data Network (AODN, https://portal.aodn.org.au/), GLODAP (https://www.glodap.info/), Palmer Long Term Ecological Record (Pal-LTER, https://pal.lternet.edu/data), Biological and Chemical Oceanography Data Management Office (BCO-DMO, https://www.bco-dmo.org/data), the CSIRO Marlin Data Trawler (Marlin, https://www.cmar.csiro.au/data/trawler/) and the Australian Antarctic Data Center (AADC, https://data.aad.gov.au/) to retrieve relevant data records.

From these data records, 'r length(all_expos)' relevant voyages were identified using unique 12-digit expedition codes (EXPOCODES) assigned as follows; National Oceanographic Data Centre (NODC) platform codes followed by voyage 8 digit start dates (YYYYMMDD). These codes are recorded on GitHub (https://github.com/KimBaldry/BIO-MATE/product_data/supporting_information/codes) and within the BIOMATE software (https://github.com/KimBaldry/BIOMATE-Rpackage/inst/codes). If the vessel name or voyage start/end dates were absent within data records, the information was obtained using the Google Search engine to discover voyage records.

The voyage records were then used to perform a wider search for published datasets in the remaining data streams. All published datasets within the first version of BIO-MATE were openly sourced throgh CCHDO (https://cchdo.ucsd.edu/), MGDS (https://www.marine-geo.org/), PANGEA, SeaBASS, AODN, GLODAP, Pal-LTER, BCO-DMO, Marlin and the AADC. If a data record was not found within these repositories, a final search using the Google search engine was performed using keywords [i.e. ship name, synonyms for voyages, year, “underway”/ “CTD”, “chlorophyll”, ”POC”, “cruise report” and “data”] to determine its absence. 

#### *Semi-automated BIO-MATE workflow for reformatting datasets*
A semi-automated workflow and the BIO-MATE R software (https://github.com/KimBaldry/BIOMATE-Rpackage) was used to reformat publish datasets, and produce the BIO-MATE data aggregate product /@ref(fig:user_guide). Downloaded data files were split by EXPOCODES if they existed in a larger dataset (e.g PALMER-LTER data records). Files for the profiling sensor data stream was further split into individual profiling events. Metadata was manually entered into a table to inform the reformatting software and a bulk run of the BIO-MARE R software was run to reformat downloaded data files. The workflow is described in  more detail in the following subsections.

##### *Download of published datasets*
Published datasets were manually downloaded from open source repositories and stored locally in accordance with described data policies. Some manual reformatting of a small portion of downloaded data had to be performed on old datasets prior to the application of reformatting scripts due to formatting irregularities. Downloaded data files, and their ammendments, used to create the BIO-MATE data product are not published within BIO-MATE, but are available upon request to the corresponding author.

##### *Splitting large datasets with BIO-MATE software*
The BIO-MATE R software requires each file to only contain observations from a single voyage. Further, the profiling sensor data stream requires each file to only contain observations from a single profiling cast, held within a descrete directory for each voyage. 

The *split_delim_file* function allows files to be split using identified variables containing EXPOCODE synonyms and/or profiling station information. This function can be used to split a single, large data file into smaller files that can be used as input in the *PROF_to_WHPE*, *PIG_to_WHPE*, *UWY_to_WHPE* and *POC_to_WHPE* BIO-MATE functions. For this version of the data product, a number of files had to be split to be ingested into BIO-MATE software. A record of these can be found in GitHub ().

##### *Data set information and processing metadata*
Information on file formats, dataset metadata, citation information, location data variables and ocean data variables are needed to reformat published datasets with BIO-MATE software. This information is called processing metadata herein, and was manually entered and stored as comma delimited text files. The proocessing metadata required to run BIO-MATE software is described in /@ref(tab:processing_metadata), and differs for each data stream. All processing metadata used to construct the BIOMATE aggregatd data product is stored in GitHub (https://github.com/KimBaldry/BIO-MATE/tree/main/product_data/processing_metadata).

##### *Dataset citation with BibTEX files*
Citation information is included within the BIO-MATE software and data product, for published datasets ingested into the BIO-MATE aggregate data product, laboratory analysis methodologies (for the PIG and POC data streams) and data repositories through which published datasets were accessed. Each citation was recorded as a BibTEX entry that allows citations to imported and managed within programs including EndNote, R and LaTEX. Each BibTEX entry has a tag that is referenced within processing metadata to link citations to their corresponding data records when datasets are ingested in the BIO-MATE software. Citation information is printed within the header information in reformatted files created by BIO-MATE software. Where possible BibTEX entries were sourced from data repositories. If BibTEX entries were not found, they were created manually.

All BibTEX entries are stored on GitHub (https://github.com/KimBaldry/BIO-MATE/product_data/supporting_information/citations) and within the BIOMATE software (https://github.com/KimBaldry/BIOMATE-Rpackage/inst/citations). A look-up table is included within the BIO-MATE software to help users find relevant BibTEX entries needed to cite datasets appropriately. A function *export_ref* supports the export of a smaller BibTEX file based on user selections of EXPOCODES and data streams that they have accessed through the product.

#### *Reformatting and linking data streams with BIO-MATE R software*
Once data files were in the appropriate format, processing metadata was entered and citation information collated, the BIO-MATE R software was run to reformat data files. The Bio-MATE R software reformats data files to the WHP-Exchange format as described in (https://exchange-format.readthedocs.io/en/latest/index.html), by drawing on processing metadata and citation information. The WHP-Exchange file format keeps metadata as multiple header names which is followed by a data table using pre-described headers. Pigment records were alternatively named according to the MAREDAT data product to conform with community expectations.

Each data stream has its own reformatting function within the BIO-MATE R software (UWY_to_WHPE,  PROF_to_WHPE, PIG_to_WHPE, POC_to_WHPE). The reformatted files are arranged into four data stream directories that include, for each EXPOCODE, 1) one WHP-Exchange file for underway sensors 2) a WHP-exchange file per profiling station for profiling sensors 3) one WHP-Exchange file for pigment measurements measured by a particular method and 4) one WHP-Exchange file for POC measurements measured by a particular method.



#### *Matching profiling data*


#### *Matching underway data*




### *Publication of the BIO-MATE data aggreggate*

#### *Quality assurance*
The initial integrity of these data records lies with the Principal Investigators of the published data record. As a result, reformatted data has varying levels of quality control and post-processing. We have included cruise report citations within our product to aid in further data quality assurance efforts. Future versions of the product may include more detailed quality assurance assessments.

# Data Records

Reformatted files are arranged into four data streams: 1) underway sensors, 2) profile sensors, 3) pigments and 4) POC. These data streams are all stored on the IMAS data portal and are linked through unique EXPOCODES. Supporting data for these data streams contains a metadata table and BibTEX citation files.

								


#### *Underway sensors*

#### *Profiling sensors*

#### *Pigments*

#### *Particulate Organic Carbon*

# Technical Validation

# Usage Notes
The community is welcome to contribute to the development of BIO-MATE software and to contribute published data to the aggregation. Outlined below are some notes on how the community can do this BIO-MATE and the intended use of the BIO-MATE aggregation in scientific studies. 

#### *Contributing to BIO-MATE software development*
It is recommended that changes to BIO-MATE be made within the GitHub environment. Contributors can fork the existing BIO-MATE repository and make changes directly to the source code. Once changes are made, they can be directed back to the BIO-MATE repository and released as an updated version of BIO-MATE. If the BIO-MATE source code is to be significantly developed, we suggest that the corresponding author is contacted and a hand-over of the software is negotiated. We encourage the addition of new data streams to BIO-MATE, the expansion of BIO-MATE capabilities, the addition of quality assurances and increases in software efficiency. 

#### *Contributing data to BIO-MATE*
Users can submit published biological ocean data to BIO-MATE using the R shiny app. Once data is submitted it can be downloaded by the user and is automatically held in the BIO-MATE GitHub repository for future addition into the product. We ask that all data submitted to BIO-MATE is published elsewhere and that users enter an accurate citation for the data they are submitting. 
For large data submissions, users can create their own workflows using the BIO-MATE R package to reformat data and information described in Figure 2. Once data has been reformatted, it can be submitted to the corresponding author via GitHub or direct communication.
Currently, BIO-MATE only supports data files stored in text-delimited formats, with structured headers and columns in a data table, and NetCDF format. The user is required to enter in some metadata to inform the software on input formats. 

#### *Recommended use in Data Analyses*
We encourage the use of the data aggregate product as a new integrated database of biological and physical data. Data files from selected voyages can be identified using unique EXPOCODES. This makes it easy to use multiple data streams in analysis, by indexing files across these EXPOCODES. Alternatively, the selection tool on the IMAS repository, or the included shapefile helps users to select voyages using spatial bounds.
Limited quality assurance has been performed on the BIO-MATE aggregate product and is highly variable across published datasets. It is recommended that users process data using their own quality assurance procedures, suitable for their unique data analysis. This allows a range of users to benefit from the BIO-MATE aggregate product and ensures data remains to the standard it was published at.

# Code Availablility
All data processing was performed in R software (Version 1.1.423). The BIO-MATE R software is freely available (https://github.com/KimBaldry/BIOMATE-Rpackage). The semi-automated workflow and accompanying processing data used to construct the data product, along with the code used to create the data decrioptor is freely accessible via Git Hub (https://github.com/KBaldry/BIO-MATE).

# Aknowledgements
This research was supported under Australian Research Council's Special Research Initiative for Antarctic Gateway Partnership (Project ID SR140300001) and a 2019 Fellowship from the Scientififc Committee of Antarctic Research. Data included in the data product was made available by the following data repositories; PANGEAE, AODN, SeaBASS, CCHDO, AADC. Records of data access dates, source addresses and digital object identifiers are recorded as metadata within the product, alongside appropriate data citations. We acknowledge the enormous community effort undertaken in the collection, analysis and publication of this data and thank principle investigators for publishing their data in open access repositories.

# Author Contributions
KB designed the data product, performed the data aggregation and wrote the manuscript. RJ contributed to the data product design and manuscript edits.

# Competing Interests
The authors of this manuscript declare no conflicts of interest.

# Figures

```{r abstract, fig.cap = "a figure"}
knitr::include_graphics("F1_BIO-MATE.png")
```


```{r method, fig.cap = "a figure"}
knitr::include_graphics("Method.png")
```

```{r user_guide, fig.cap = "a figure"}
knitr::include_graphics("BIOMATE_usage.png")
```


# Figure Legends

# Tables

```{r processing_metadata, tab.cap = "a figure"}
p_meta = data.frame("variable"= character(), "stream" = character(), "description" = character(), "input"= character())
p_meta = p_meta %>% 
  add_row(variable = "file_type", stream = "all", description = "The format of the file/s.", input = "text delim or netcdf") %>%   
  add_row(variable = "path", stream = "all", description = "A path to where the file/s is stored.", input = "A pathname that is R compatible") %>%   
  add_row(variable = "extention", stream = "all", description = "The extension of the file/s.", input = "text delim or netcdf") %>%   
  add_row(variable = "delim", stream = "all", description = "Only fill if rectangular text-delimited file/s.", input = "rect") %>%  
  add_row(variable = "header_sep", stream = "all", description = "A separator used in headers of the file. Headers often store location data in profiling datasets and need extraction. Can be left empty.", input = "colon, comma, dash, equals, space") %>%  
  add_row(variable = "missing_value", stream = "all", description = "The value or character used to indicate a missing value.", input = "value") %>%  
  add_row(variable = "not_detected", stream = "PIG, POC", description = "The value or character used to indicate a variable was not detected in analysis.", input = "value") %>%  
  
  add_row(variable = "EXPOCODE", stream = "all", description = "The EXPOCODE of the voyage associated with the data.", input = "12-digit code") %>%
  add_row(variable = "source", stream = "all", description = "The data repository the data files were sourced from.", input = "The short name of the data repository used within BIO-MATE. See https://github.com/KimBaldry/BIO-MATE/product_data/supporting_information/BIOMATE_SOURCES.txt.") %>%
  add_row(variable = "PI", stream = "all", description = "The principle investigator/s responsible for the published dataset.", input = "Names sepparated by a dash") %>%
    add_row(variable = "Institution", stream = "all", description = "The institution/s who collected the data.", input = "Names sepparated by a dash") %>%
  add_row(variable = "contact", stream = "all", description = "A contact for the published dataset.", input = "E-mail address") %>%
  add_row(variable = "citation", stream = "all", description = "The BIO-MATE citation tag/s used to reference a BibTEX entry for the published dataset.", input = "A BIO-MATE citation tag") %>%
  add_row(variable = "analysis_type", stream = "PIG, POC", description = "The type of analysis used on water samples for the published dataset.", input = "A code to reference an analysis type. See https://github.com/KimBaldry/BIO-MATE/product_data/supporting_information/BIOMATE_METHODS.txt") %>%
  add_row(variable = "Method", stream = "PIG, POC", description = "The method used to analyse water samples for the published dataset.", input = "A code to reference a method. See https://github.com/KimBaldry/BIO-MATE/product_data/supporting_information/BIOMATE_METHODS.txt") %>%
  
  add_row(variable = "TZ", stream = "all", description = "The time zone for date and time information.", input = "Time zone code") %>%
  add_row(variable = "STNNBR", stream = "all", description = "The name of the variable for the station number of the profiling staation.", input = "Text. If recorded in header use header-[variable].") %>%
  add_row(variable = "CASTNO", stream = "all", description = "The name of the variable for the cast number at the profiling station.", input = "Text. If recorded in header use header-[variable].") %>%
  add_row(variable = "DATE", stream = "PROF", description = "The name of the variable for the date of the profiling cast.", input = "Text. If recorded in header use header-[variable].") %>%
  add_row(variable = "DATE_analyser", stream = "PIG, POC", description = "The name of the variable for date of observation recorded by the analyser.", input = "Text. If recorded in header use header-[variable].") %>%
  add_row(variable = "DATE_format", stream = "PROF", description = "Format for DATE.", input = "A format string code. See strptime in R for codes.") %>%
  add_row(variable = "DATE_analyser_format", stream = "PIG, POC", description = "Format for DATE_analyser.", input = "A format string code. See strptime in R for codes.") %>%
  add_row(variable = "TIME_s", stream = "PROF", description = "The name of the variable for time at the start of teh profiling cast.", input = "Text. If recorded in header use header-[variable].") %>%
  add_row(variable = "TIME_b", stream = "PROF", description = "The name of the variable for time at the bottom of the profiling cast.", input = "Text. If recorded in header use header-[variable].") %>%
  add_row(variable = "TIME_e", stream = "PROF", description = "The name of the variable for time at the end of the profiling cast.", input = "Text. If recorded in header use header-[variable].") %>%
  add_row(variable = "TIME_analyser", stream = "PIG, POC", description = "The name of the variable for time of observation recorded by the analyser.  If recorded in header use header-[variable].", input = "Text") %>%
  add_row(variable = "TIME_format", stream = "PROF", description = "Format for TIME.", input = "A format string code. See strptime in R for codes.") %>%
  add_row(variable = "TIME_b_format", stream = "PROF", description = "Format for TIME_b, if different to TIME_format.", input = "A format string code. See strptime in R for codes.") %>%
  add_row(variable = "TIME_analyser_format", stream = "PIG, POC", description = "Format of TIME_analyser.", input = "A format string code. See strptime in R for codes.") %>%

  add_row(variable = "LATITUDE_s", stream = "PROF", description = "The name of the variable for latitude at the start of the profiling cast.", input = "Text") %>%
  add_row(variable = "LATITUDE_b", stream = "PROF", description = "The name of the variable for latitude at the bottom of the profiling cast.", input = "Text") %>%
  add_row(variable = "LATITUDE_e", stream = "PROF", description = "The name of the variable for latitude at the end of the profiling cast.", input = "Text") %>%
  add_row(variable = "LONGITUDE_s", stream = "PROF", description = "The name of the variable for longitude at the start of the profiling cast.", input = "Text") %>%
  add_row(variable = "LONGITUDE_b", stream = "PROF", description = "The name of the variable for longitude at the bottom of the profiling cast.", input = "Text") %>%
  add_row(variable = "LONGITUDE_e", stream = "PROF", description = "The name of the variable for longitude at the end of the profiling cast.", input = "Text") %>%
  add_row(variable = "LAT_analyser", stream = "PIG, POC", description = "The name of the variable for latitude recorded by the analyser.", input = "Text") %>%
  add_row(variable = "LON_analyser", stream = "PIG, POC", description = "The name of the variable for longitude recorded by the analyser.", input = "Text") %>%
  add_row(variable = "POSITION_format", stream = "all", description = "The format of latitude and longitude data.", input = "A string describing the format made up of %deg (degrees), %min (minutes), %sec (seconds) and %pos (for N/S/E/W specification)") %>%
  add_row(variable = "Sample_ID", stream = "PIG, POC", description = "The name of the variable containing sample identification.", input = "Text") %>%
  add_row(variable = "BOTTLE", stream = "PIG, POC", description = "The name of the variable containing bottle identifications.", input = "Text") %>%
  add_row(variable = "Underway_ID", stream = "PIG, POC", description = "How underway samples are identified within the dataset. Leave blank if there are no underway values within the dataset.", input = "[variable name]-[value] or all") %>%  
  
  add_row(variable = "CTDPRS", stream = "PROF", description = "The name of the variable for pressure collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDPRS_u", stream = "PROF", description = "The units for pressure collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDTMP", stream = "PROF", description = "The name of the variable for temperature collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDTMP_u", stream = "PROF", description = "The units for temperature collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDSAL", stream = "PROF", description = "The name of the variable for salinity collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDSAL_u", stream = "PROF", description = "The units for salinity collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDOXY", stream = "PROF", description = "The name of the variable for oxygen collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDOXY_u", stream = "PROF", description = "The units for oxygen collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDFLUOR", stream = "PROF", description = "The name of the variable for fluorescence collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDFLUOR_u", stream = "PROF", description = "The units for fluorescence collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDBEAMCP", stream = "PROF", description = "The name of the variable for beam attenuation collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDBEAMCP_u", stream = "PROF", description = "The units for beam attenuation collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDBBP700", stream = "PROF", description = "The name of the variable for optical backscatter (700 nm) collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDBBP700_u", stream = "PROF", description = "The units for optical backscatter (700 nm) collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDXMISS", stream = "PROF", description = "The name of the variable for transmittance collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDXMISS_u", stream = "PROF", description = "The units for transmittance collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDPAR", stream = "PROF", description = "The name of the variable for photosyntheitically active radiation collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDPAR_u", stream = "PROF", description = "The units for photosyntheitically active radiation collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDNITRATE", stream = "PROF", description = "The name of the variable for oxygen collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDNITRATE_u", stream = "PROF", description = "The units for oxygen collected by the profiling sensor.", input = "Text") %>%
  
  add_row(variable = "DEPTH", stream = "PIG,POC", description = "The name of the variable for depth of observation recorded by the analyser.", input = "Text") %>%
  add_row(variable = "PIG_u", stream = "PIG", description = "The units for pigment measurements recorded by the analyser.", input = "Text") %>%
  add_row(variable = "FCHLORA", stream = "PIG", description = "The name of the variable for fluorometrically derived chlorophyll.", input = "Text") %>%
  add_row(variable = "FPHEO", stream = "PIG", description = "The name of the variable for fluorometrically derived phaeopigments.", input = "Text") %>%
  add_row(variable = "FPHYTIN", stream = "PIG", description = "The name of the variable for fluorometrically derived phaeophytin.", input = "Text") %>%
  add_row(variable = "TCHLA", stream = "PIG", description = "The name of the variable for HPLC derived total chlorophyll a.", input = "Text") %>%
  add_row(variable = "TACC", stream = "PIG", description = "The name of the variable for HPLC derived total accessory pigments.", input = "Text") %>%
  add_row(variable = "DVChla", stream = "PIG", description = "The name of the variable for HPLC derived divinyl chlorophyll a.", input = "Text") %>%
  add_row(variable = "Chla", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll a.", input = "Text") %>%
  add_row(variable = "Chla_ide", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyllide.", input = "Text") %>%
  add_row(variable = "Chla_allom", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll a allomers.", input = "Text") %>%
  add_row(variable = "Chla_prime", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll a prime.", input = "Text") %>%
  add_row(variable = "Chlb", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll b.", input = "Text") %>%
  add_row(variable = "DVChlb", stream = "PIG", description = "The name of the variable for HPLC derived divinyl chlorophyll b.", input = "Text") %>%
  add_row(variable = "Chlc", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll c.", input = "Text") %>%
  add_row(variable = "Chlc1_Chlc2_Mg_3_8_divinyl_pheoporphyrin_a5", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll c1 + chlorophyll c2 + Mg 3,8 divinyl pheoporphyrin a5.", input = "Text") %>%
  add_row(variable = "Chlc1", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll c1.", input = "Text") %>%
  add_row(variable = "Chlc1_like", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll c1-like.", input = "Text") %>%
  add_row(variable = "Chlc2", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll c2.", input = "Text") %>%
  add_row(variable = "Chlc1_Chlc2", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll c1 + chlorophyll c2.", input = "Text") %>%
  add_row(variable = "Chlc3", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll c3.", input = "Text") %>%
  add_row(variable = "MgDVP", stream = "PIG", description = "The name of the variable for HPLC derived Mg 2,4 divinyl pheoporphyrin a5 monomethyl ester.", input = "Text") %>%
    add_row(variable = "19Hex", stream = "PIG", description = "The name of the variable for HPLC derived 19’hexanoyloxyfucoxanthin.", input = "Text") %>%
  add_row(variable = "19But", stream = "PIG", description = "The name of the variable for HPLC derived 19’butanoyloxyfucoxanthin.", input = "Text") %>%
    add_row(variable = "Fucox", stream = "PIG", description = "The name of the variable for HPLC derived fucoxanthin.", input = "Text") %>%
  add_row(variable = "Perid", stream = "PIG", description = "The name of the variable for HPLC derived peridinin.", input = "Text") %>%
    add_row(variable = "Prasino", stream = "PIG", description = "The name of the variable for HPLC derived prasinoxanthin.", input = "Text") %>%
  add_row(variable = "Allox", stream = "PIG", description = "The name of the variable for HPLC derived alloxanthin.", input = "Text") %>%
    add_row(variable = "Lutein", stream = "PIG", description = "The name of the variable for HPLC derived lutein.", input = "Text") %>%
  add_row(variable = "Zeax", stream = "PIG", description = "The name of the variable for HPLC derived zeaxanthin.", input = "Text") %>%
    add_row(variable = "Zea_Lut", stream = "PIG", description = "The name of the variable for HPLC derived zeaxanthin + lutein.", input = "Text") %>%
  add_row(variable = "Violax", stream = "PIG", description = "The name of the variable for HPLC derived violaxanthin.", input = "Text") %>%
    add_row(variable = "Alpha_car", stream = "PIG", description = "The name of the variable for HPLC derived alpha carotene.", input = "Text") %>%
  add_row(variable = "Beta_car", stream = "PIG", description = "The name of the variable for HPLC derived beta carotene.", input = "Text") %>%
    add_row(variable = "Gamma_car", stream = "PIG", description = "The name of the variable for HPLC derived gamma carotene.", input = "Text") %>%
  add_row(variable = "Epsilon_car", stream = "PIG", description = "The name of the variable for HPLC derived epsilon carotene.", input = "Text") %>%
      add_row(variable = "Alpha_Beta_car", stream = "PIG", description = "The name of the variable for HPLC derived alpha + beta carotene.", input = "Text") %>%
  add_row(variable = "Neox", stream = "PIG", description = "The name of the variable for HPLC derived neoxanthin.", input = "Text") %>%
      add_row(variable = "DD", stream = "PIG", description = "The name of the variable for HPLC derived diadinoxanthin.", input = "Text") %>%
  add_row(variable = "DT", stream = "PIG", description = "The name of the variable for HPLC derived diatoxanthin.", input = "Text") %>%
      add_row(variable = "Viol_Neox", stream = "PIG", description = "The name of the variable for HPLC derived violaxanthin + neoxanthin.", input = "Text") %>%
  add_row(variable = "Phaeopigments", stream = "PIG", description = "The name of the variable for HPLC derived bulk phaeopigments.", input = "Text") %>%
        add_row(variable = "Phide_a", stream = "PIG", description = "The name of the variable for HPLC derived phaeophorbide a.", input = "Text") %>%
  add_row(variable = "Phytin_a", stream = "PIG", description = "The name of the variable for HPLC derived phaeophytin a.", input = "Text")
  
colnames(p_meta) = c("Processing metadata variable", "Data stream/s","Description", "Input Guide" )  
  
knitr::kable(p_meta) %>% kable_paper("striped", full_width = F) %>% pack_rows("File format information", 1,7)  %>% pack_rows("Data aquisition information", 8,15) %>% pack_rows("Location data information", 16,41) %>%  pack_rows("Profiling sensor data information", 42,61) %>% pack_rows("Pigment and POC data information", 62,105)

save(p_meta, file = "metadata_info.RData")
```


# References
