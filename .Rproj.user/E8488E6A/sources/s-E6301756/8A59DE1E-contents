---
title: 'BIO-MATE: A biological ocean data reformatting effort'
author:
  affiliation: |
    $*$: Corresponding author. E-mail: kimberlee.baldry@utas.edu.au
    $a$: Institute for Marine and Antarctic Studies, University of Tasmania, Hobart, Australia
    $b$: Bureau National Operations Centre, Bureau of Meteorology, Hobart, Australia
  name: |
    [Kimberlee Baldry](kimberlee.baldry@utas.edu.au) <a href="https://orcid.org/0000-0003-3286-8624"><img alt="ORCID logo" src="https://info.orcid.org/wp-content/uploads/2019/11/orcid_16x16.png" width="16" height="16" /></a> $^{*,a}$
    Robert Johnson <a href ="https://orcid.org/0000-0002-5915-5416"><img alt="ORCID logo" src="https://info.orcid.org/wp-content/uploads/2019/11/orcid_16x16.png" width="16" height="16" /></a> $^{b}$
    Peter G. Strutton <a href="https://orcid.org/0000-0002-2395-9471"><img alt="ORCID logo" src="https://info.orcid.org/wp-content/uploads/2019/11/orcid_16x16.png" width="16" height="16" /><a/> $^{a}$
    Philip W. Boyd <a href="https://orcid.org/0000-0001-7850-1911"><img alt="ORCID logo" src="https://info.orcid.org/wp-content/uploads/2019/11/orcid_16x16.png" width="16" height="16" /><a/> $^{a}$
bibliography: descriptor.bib
output:
  bookdown::word_document2:
    fig_caption: yes
    number_sections: no
csl: nature.csl
always_allow_html: true
abstract: "Biological ocean data collected from ships find reuse in aggregations
  of historical data. These data are heavily relied upon to document long term change,
  validate satellite algorithms for ocean biology and are useful in assessing the
  performance of autonomous platforms and biogeochemical models. \nThere is a need to
  combine subsurface biological and physical data into one aggregate data product
  to support reproducible research. Existing aggregate products
  are dissimilar in source data, have largely been isolated to the surface ocean and
  most omit physical data. These products cannot easily be used to explore subsurface
  bio-physical relationships.\nWe present the first version of a biological ocean
  data reformatting effort (BIO-MATE, https://gitlab.com/KBaldry/BIO-MATE). BIO-MATE uses R software that reformats
  openly sourced published datasets from oceanographic voyages.
  These reformatted biological and physical data from underway sensors, profiling sensors, pigments
  analysis and particulate organic carbon analysis are stored in an interoperable and reproducible BIO-MATE data product for easy access and use ([AODNlink]).\n"

---

```{r, include=FALSE}
library(kableExtra)
library(tidyverse)
library(flextable)
library(gridExtra)
source("./Create_vars.R")
```


# Background & Summary
Marine phytoplankton blooms support ocean food-webs and influence global climate through the biological carbon pump (@schmitt2018; @ainley1991: @basu2018). Ocean physics and other environmental drivers control the timing, magnitude and extent of phytoplankton blooms through complex bio-physical relationships (@carranza2018; @prairie2012; @wihsgott2019; @brody2015). To study these relationships integrated data structures that link biological and physical ocean data are needed. Ship-based data are the gold standard for accurate biological oceanographic measurements (@mignot2019). These data are often published separately to physical ocean data, stored across different repositories and in multiple formats. This makes it difficult and time-consuming to aggregate and link biological and physical data. The described data product attempts to make this task easier.

The biological ocean data reformatting effort (BIOMATE) works to link existing, open-access biological and physical datasets across oceanographic voyages and promote their re-use. This has been done by developing a BIOMATE R software package that not only reformats published datasets, but also cross-references between biological and physical data and allows access to citation information (https://git hub.com/KimBaldry/BIOMATE-Rpackage). The resulting BIOMATE data product allows users to easily access, manipulate and cite published ship-based datasets of different dimensions for multiple applications. 

The BIO-MATE data product can be accessed via the Australian Ocean Data Network (https://portal.aodn.org.au/). The aggregation includes data collected from shipboard underway sensors, profiling sensors mounted on sampling rosettes, lab analysis for phytoplankton pigments and lab analysis for particulate organic carbon (POC). These data are stored as four data streams, cross-referenced by unique expedition codes (EXPOCODE) and profiling station identifications (CTD_ID). An additional data stream contains supporting information for the data product including a list of oceanographic voyages, investigator contact information and data citations for reformatted datasets. We have also included an aggregated data table for biological data. Users are required to refer to supporting data and cite all data products accessed through BIO-MATE, as well as the BIO-MATE data product itself. We consulted the distribution licenses of all data sources to ensure that with this condition data are re-used lawfully.


The data product is currently has been used to address discrepancies between Southern Ocean satellite and in-situ measurements (Johnson *et al.* in prep), to understand how the response of in-situ fluorometers changes in the Southern Ocean (Baldry *et al*. in prep), and to investigate the role of ocean physics in mediating subsurface chlorophyll features (Baldry *et al.* in prep). These examples highlight the malleability of this data product to improve our understanding of biological oceanography in the Southern Ocean. Further uses include validating satellite observations (@valente2019; @johnson2013), developing new ways to validate in-situ bio-optical observations collected by autonomous profiling platforms in the presence of dynamic fronts (@sauzede2015b; @roesler2017; @mignot2019), training ocean state estimations (@verdy2017), informing bio-physical models () and using multi-variate analyses to understand bio-physical relationships ().

We recognise the massive effort in producing the thousands of data records in this data product. This includes the investigators and data officers who have spent countless hours in ship time, project organisation, grant writing, laboratory analysis, data processing and report writing. Oceanographic data are often collected with regional studies in mind, but their value increases with publication and re-use. We encourage all investigators to publish their data for re-use through data products like BIO-MATE.

# Methods

#### *Published datasets in BIO-MATE* 
The BIO-MATE aggregate data product brings together ship-based data that have been collected by a Principal Investigator (PI) and openly published by a to a publicly accessible data (Figure \@ref(fig:method)). The first version of BIO-MATE includes published datasets associated with four types of measurements:
1. sensors in the vessels underway seawater in-take  (underway sensor data stream),  
2. profiling sensors mounted to sampling rosettes (profiling sensor data stream),  
3. pigments measured in the laboratory (pigment data stream),   
and 4. POC measured in the laboratory (POC data stream).  

Data records from the pigment data stream were first identified in data repositories that host biological data (November 2019). Pigment data records were identified using the search term “chlorophyll” and a latitude bound of 30 $^\circ$S to 90 $^\circ$S from PANGAEA (https://www.pangaea.de/), SeaBASS (https://seabass.gsfc.nasa.gov/), the Australian Ocean Data Network (AODN, https://portal.aodn.org.au/), GLODAP (https://www.glodap.info/), the Palmer Long Term Ecological Record (Pal-LTER, https://pal.lternet.edu/data), the Biological and Chemical Oceanography Data Management Office (BCO-DMO, https://www.bco-dmo.org/data), the CSIRO Marlin Data Trawler (Marlin, https://www.cmar.csiro.au/data/trawler/) and the Australian Antarctic Data Center (AADC, https://data.aad.gov.au/). Data records from the profiling sensors and underway sensors data streams were then identified in these repositories and in the CCHDO (https://cchdo.ucsd.edu/) and MGDS (https://www.marine-geo.org/).

From pigment data records, `r length(expo_info$All) ` relevant voyages were identified using unique 12-digit expedition codes (EXPOCODES) assigned as follows; National Oceanographic Data Centre (NODC) platform codes followed by voyage 8 digit start dates (YYYYMMDD). NODC platform and country codes are recorded on Git Hub (https://github.com/KimBaldry/BIO-MATE/product_data/supporting_information/codes) and within the BIOMATE software (https://github.com/KimBaldry/BIOMATE-Rpackage/inst/codes). If the vessel name or voyage start/end dates were absent, this information was found using Google to discover voyage records. This voyage information was used to do a final Google keywords search (i.e. ship name, synonyms for voyages, year, “underway”/ “CTD”, “chlorophyll”, ”POC”, “cruise report” and “data”) to determine if any absent records were findable and find accompanying cruise reports.

#### *Semi-automated BIO-MATE workflow for reformatting datasets*
A semi-automated workflow and the BIO-MATE R software (https://github.com/KimBaldry/BIOMATE-Rpackage) were used to reformat published datasets, and produce the BIO-MATE data product (Figure \@ref(fig:userguide)). Downloaded data files were split by EXPOCODES if they recorded data within a larger dataset (e.g PALMER-LTER data records). Files for the profiling sensor data stream were further split into individual profiles. Processing metadata were manually entered into a table to inform the BIO-MATE R software and a bulk run of the software was performed to reformat data files. The workflow is described in  more detail in the following subsections (Figure \@ref(fig:method)).

##### *Download of published datasets*
Published datasets were manually downloaded from open source repositories and stored locally in accordance with data policies. Some manual reformatting of a small portion of downloaded data had to be performed on old datasets, prior to the application of reformatting scripts, due to formatting irregularities. Downloaded data files, and their amendments, used to create the BIO-MATE data product are not published in BIO-MATE, but are available upon request to the corresponding author.

##### *Splitting large datasets with BIO-MATE software*
The BIO-MATE R software requires each file to only contain observations from a single voyage. Further, the profiling sensor data stream requires each file to only contain observations from a single profiling cast, held in a discrete directory for each voyage. 

The *split_delim_file* function splits files using identified variables containing EXPOCODE synonyms and/or profiling station information. This function can be used to split a single, large data file into smaller files as required. For this version of the data product, a number of files had to be split to be ingested into the BIO-MATE core functions. A record of these can be found in Git Hub ().

##### *Processing metadata*
Information on file formats, dataset information, citation information, location data variables and ocean data variables are needed to reformat published datasets with BIO-MATE software. This information is called processing metadata herein, and was manually entered and stored as comma delimited text files. The processing metadata required to run BIO-MATE software is described in Table \@ref(tab:processingmetadata), and differs for each data stream. All processing metadata used to construct the BIOMATE aggregated data product is stored in Git Hub (https://github.com/KimBaldry/BIO-MATE/tree/main/product_data/processing_metadata).

##### *Dataset citation with BibTEX files*
Information is included in the BIO-MATE data product, for citing published datasets, laboratory analysis methodologies (for the PIG and POC data streams) and the data repositories through which published data records were accessed. Each citation was recorded as a BibTeX entry, compatible with EndNote, R and LaTeX. Each BibTeX entry has a tag that is referenced in the processing metadata. This tag is used to link citations to their corresponding data records when datasets are ingested in the BIO-MATE software. Citation information is then printed in the header information in reformatted files. Where possible BibTeX entries were sourced from data repositories. If BibTeX entries were not found, they were created manually.

All BibTeX entries are stored on Git Hub (https://github.com/KimBaldry/BIO-MATE/product_data/supporting_information/citations) and in the BIOMATE software (https://github.com/KimBaldry/BIOMATE-Rpackage/inst/citations). A look-up table is included in the BIO-MATE software to help users find relevant BibTeX entries needed to cite datasets appropriately (). A function *export_ref* supports the export of a smaller BibTeX file based on user selections of EXPOCODES and data streams that they have accessed through the product.

##### *Reformatting and linking data streams with BIO-MATE R software*
The BIO-MATE R software was run to reformat data files to the WHP-Exchange format  (https://exchange-format.readthedocs.io/en/latest/index.html), using the original or split data files, processing metadata and citation information as input. The software arranges reformatted WHPE files into four data streams in local directories that include separate WHPE files, for each EXPOCODE, and for underway sensors, profiling sensor casts, pigment measurements, and POC measurements.

Each data stream has its own reformatting function within the BIO-MATE R software (*UWY_to_WHPE*, *PROF_to_WHPE*, *PIG_to_WHPE*, *POC_to_WHPE*). The software requites physical (underway sensor and profiling sensor) data streams to be reformatted before biological (pigment and POC) data streams to accommodate a biological-physical matching algorithm within the PIG_to_WHPE and POC_to_WHPE functions. The algorithm links biological data in the pigment and POC data streams to the physical data in the profiling sensor and underway sensor data streams. Biological data records are given a profiling sensor identification tag (CTD_ID) if matched to physical data in BIO-MATE.

To match biological data to physical data, the algorithm first uses EXPOCODES to find relevant physical data in profiling sensor data streams. It then matches biological and physical data records by comparing station number (STNBR) and cast number (CASTNO) records. If matches are detected using STNBR and CASTNO, the validity of these matches is checked by comparing time and position information. If position and time were not recorded in biological datasets, it is assumed that the STNNBR and CASTNO records are correct. Otherwise, a match is recorded if both the biological and physical data record data either within 24 hours of each other or within 8 km (@johnson2017). After this routine, if unmatched biological data still exist, a database of time and position information from all profiling sensor data relating to the EXPOCODE is created. Matches are found for biological data, if it contains position information, by finding the closest profiling sensor record within 1km in the database. If time information exits, matches are identified as the closest profiling sensor record within 6 hours, otherwise only matching date information is required.

[underway matching info if developed]


#### *Quality assurance*
Limited quality assurance has been performed on the BIO-MATE data product and is variable across published datasets. The initial integrity of these data records lies with the Principal Investigators of the published data record. As a result, reformatted data have varying levels of quality control and post-processing. We have included cruise report citations in our product to aid in further data quality assurance efforts.

This allows a range of users to benefit from the BIO-MATE aggregate product and ensures data remains to the standard it was published at. The quality assurance required of physical and biological ocean data varies according to application, and is up to the user to confirm the data is suitable for their application. For example, when assessing basin-scale trends, a lower level of quality assurance is required, compared to when validating or training satellite algorithms or ocean models. Future versions of BIO-MATE could implement quality assurance metrics.


# Data Records
The four data streams are all stored on the IMAS data portal (https://data.imas.utas.edu.au/portal/search), linked through unique EXPOCODES. Supporting data contains a metadata table and BibTeX citation files. The spatial extent of the data records is confined largely to the Southern Ocean, and was collected from XXXX - XXXX (Figure \@ref(fig:spatiotemporaldist)). A summary of the data records in the BIO-MATE aggregate data product is presented in Table \@ref(tab:datarecordsummary). 

#### *Underway sensor data stream*
The underway sensor data stream contains a comma delimited WHP-Exchange file for each voyage ([EXPOCODE]_UWY.csv). The format of this file consists of headers to store metadata, followed by a data table that reports records collected by underway sensors mounted on the vessel (Table \@ref(tab:uwyrecords)). 

#### *Profiling sensor data stream*
The profiling sensor data stream contains a comma delimited WHP-Exchange file for each unique profiling cast conducted on each voyage ([EXPOCODE]_[station number]_[cast number]_ctd1.csv). The file is formatted to store metadata as headers which is followed by the data table that reports records from profiling sensors mounted on a sampling rosette (Table \@ref(tab:profrecords)). 

#### *Pigment data stream*
The pigment data stream contains a comma delimited WHP-Exchange file for each voyage (named [EXPOCODE]_PIG_[SOURCED_FROM]_[METHOD].csv).  The format of this file consists of headers to store supporting information, followed by a data table that records measurements from the lab analysis of seawater samples for pigments performed by principle investigators (Table \@ref(tab:pigrecords)). 

#### *Particulate Organic Carbon data stream*
The POC data stream contains a comma delimited WHP-Exchange file for each voyage (named [EXPOCODE]_POC_[SOURCED_FROM]_[METHOD].csv). The format of this file consists of headers to store supporting information, followed by a data table that records measurements from the lab analysis of seawater samples for particulate organic carbon performed by principle investigators (Table \@ref(tab:pocrecords)). 

#### *Supporting data* 
Supporting data are included in the BIO-MATE aggregate data product to support the correct citation of data and guide user access to data. This data includes 1. a BibTeX file, that contains information to reference all BIO-MATE data records 2. an index table indicating data availability and citation tags against data records listed by EXPOCODE, data stream, method and source, 3. a records table for all data repositories from which BIO-MATE data was sourced from and 4. a records table for all pigment and POC analysis methods used in BIO-MATE data.

# Technical Validation
We validated the quality of the BIO-MATE data compilation, by displaying a number of key data distributions and trends. This validation does not confirm the quality of individual data points, in which the authors have placed no additional quality assurance to the published datasets.

The location data associated with the published datasets has been interpreted correctly by the software. This is evident from the success of the biophysical matching algorithm, along with the spatial distribution of the data and recorded sampling depths (Figure \@ref(fig:spatiotemporaldist)). The data are spread over `r min(pig_data$YYYY, na.rm = T)` - `r max(pig_data$YYYY, na.rm = T)` and predominantly collected in the month of January. This is consistent with the fact that ship-based sampling in the Southern Ocean is conducted during Austral summer, and displays a lag time in publishing most recent datasets to data repositories. All data are in the ocean, not on land, confirming the absence of spurious location data, and most samples are located in the Southern Ocean which is consistent with our search constraints. Finally information on sampling time of ship-based biological data is as expected, and CTD sampling times (start, bottom and end) are sequential and follow a trend with sampling depth (Figure \@ref(fig:CTDdist)).

The biological ocean data associated with the published datasets has been interpreted correctly by the software. Over-all fluorometrically derived chlorophyll (FCHLORA), HPLC derived chlorophyll a (Chl a) and HPLC derived total chlorophyll (TCHLA) measurements show a log-normal distribution, as expected. High values (>20 \u03BCg/l) are constrained to the coastal zones as expected (Figure \@ref(fig:chldist)). There is a linear relationship between chlorophyll-a derived from HPLC methods and chlorophyll derived from 
fluorometric methods (Figure \@ref(fig:hplcvsfluor)). This is expected, although considerable variability is expected due to the influence of phaeopigments and other accessory pigments on fluorometrically derived chlorophyll measurements. 

Five fluorometric methods to derive chlorophyll have coincident HPLC measurements. Briefly, the ANTXVIII_2 and JGOFS method shows good correlation between the two. The PALMER_LTER method shows considerable variability between methods. This may be due to the coastal location of most samples and the influence of accessory pigments, but further investigation is needed. Finally, only a small number of records were collected with Mueller *et al.* (2003) and unknown fluorometric methods (< X), making it difficult to assess the quality of the measurements.

The physical ocean data associated with published datasets has been interpreted correctly by the software. Temperature and salinity ranges fall within expected vales for the ocean, and display expected trends with latitude  (Figure\@ref(fig:physvariabledist)).

# Usage Notes
The community is welcome to contribute to the development of BIO-MATE software and to contribute published data to the aggregation, by following a user guide
(Figure \@ref(fig:userguide)).

#### *Contributing to BIO-MATE software development*
It is recommended that changes to BIO-MATE software be made through Git Hub. Contributors can fork the existing repository (https://github.com/KimBaldry/BIOMATE-Rpackage) and make changes directly to the source code. Once changes are made, they can be directed back to the BIOMATE R package repository and released as an updated version of the BIO-MATE software. If the BIO-MATE source code is to be significantly developed, we suggest that the corresponding author is contacted and a hand-over of the software is negotiated. We encourage the addition of new data streams to BIO-MATE, the expansion of BIO-MATE capabilities, the addition of quality assurances and increases in software efficiency.

#### *Contributing data to BIO-MATE*
Users can submit published biological ocean data to BIO-MATE using the R shiny app BIO-SHARE (Figure \@ref(fig:userguide)). Once data are submitted they can be downloaded by the user and automatically submitted to the BIO-MATE Git Hub repository for future addition into the product. We ask that all data submitted to BIO-MATE are published elsewhere and that users enter an accurate citation for the data they are submitting. 

For large data submissions, users can create their own workflows using the BIO-MATE R package to reformat data and information (Figure \@ref(fig:userguide)). Once data have been reformatted, they can be submitted to the corresponding author via Git Hub or direct communication.

Currently, BIO-MATE only supports data files stored in text-delimited formats, with structured headers and columns in a data table, and NetCDF format. The user is required to enter in some metadata to inform the software on input formats. 

#### *Recommended use in data analyses*
We encourage the use of the data aggregate product as a new integrated database of biological and physical data. Data files from selected voyages can be identified using unique EXPOCODES and CTD_IDs. This makes it easy to use multiple data streams in analysis, by indexing files across these EXPOCODES. Alternatively, the selection tool on the IMAS repository helps users to select voyages using spatial bounds.

# Code Availability
All data processing was performed in R software (Version 1.1.423). The BIO-MATE R software is freely available (https://github.com/KimBaldry/BIOMATE-Rpackage). The semi-automated workflow and accompanying processing data used to construct the data product, along with the code used to create the data descriptor is freely accessible via Git Hub (https://github.com/KBaldry/BIO-MATE).

# Acknowledgements
This research was supported under Australian Research Council's Special Research Initiative for Antarctic Gateway Partnership (Project ID SR140300001) and a 2019 Fellowship from the Scientific Committee of Antarctic Research. It was also partially supported by Geoscience Australia, in the form of funded study leave. Data included in the data product were made available by the following data repositories; PANGAEA, AODN/IMOS, SeaBASS, CCHDO, AADC, GLODAP, PAL-LTER, CSIRO, MDGS and BCO-DMO. Records of data access dates, source addresses and digital object identifiers are recorded as metadata, alongside appropriate data citations. We acknowledge the enormous community effort undertaken in the collection, analysis and publication of this data and thank principle investigators for publishing their data in open access repositories.

# Author Contributions
KB designed the data product, performed the data aggregation and wrote the manuscript. RJ contributed to the data product design and manuscript edits.

# Competing Interests
The authors of this manuscript declare no conflicts of interest.

# Figures

```{r,include = FALSE, message = FALSE, warning=FALSE}
library(ggplot2)
library(mapproj)
library(raster)
library(ggpubr)
library(sf)
library(rgdal)
library(dplyr)
library(SOmap)
#library(ggnewscale)
#library(ggmap)
library(scales)
library(viridis)
library(marmap)
library(sp)
library(maptools)
library(rgeos)
library(polyclip)
library(gridExtra)
library(grid)
library(lattice)
#library(GISTools)
#library(animation)
library(gridExtra)
#library(gam)
```


```{r}
pig_data$SA = unlist(lapply(1:nrow(pig_data),FUN = function(x){
  if(!is.na(pig_data$DATE_analyser[x]) & !is.na(pig_data$TIME_analyser[x]) & !is.na(pig_data$LAT_analyser[x]) & !is.na(pig_data$LON_analyser[x])){SA = sunAngle( t = as.POSIXct(paste(pig_data$DATE_analyser[x], pig_data$TIME_analyser[x]), tz = "UTC"), longitude = pig_data$LON_analyser[x], latitude = pig_data$LAT_analyser[x])$altitude}else{SA = NA}
  SA
}))
pig_data = pig_data %>% mutate(dd = as.numeric(substr(DATE_analyser, 9,10)), MM = as.numeric(substr(DATE_analyser,6 ,7)), YYYY = as.numeric(substr(DATE_analyser, 1, 4)), HH = as.numeric(substr(TIME_analyser, 1,2)))
pig_data$DEPTH[pig_data$DEPTH %in% c("uw","Underway", "underway","surface","ice","BUCKET")] = 0
pig_data$DEPTH = as.numeric(pig_data$DEPTH)
pig_data$DEPTH[pig_data$STNNBR == "U" & is.na(pig_data$DEPTH)] = 0 
pig_data$DEPTH[pig_data$DEPTH < 0] = 0

CTD_df = CTD_df %>% mutate(dd = as.numeric(substr(DATE, 9,10)), MM = as.numeric(substr(DATE,6 ,7)), YYYY = as.numeric(substr(DATE, 1, 4)))


```


```{r abstract, fig.cap = "The BIO-MATE concept for creating a consistent data compilation from existing ship-based oceanographic data", echo = FALSE, message = FALSE, warning=FALSE}
knitr::include_graphics("F1_BIO-MATE.png")
```


```{r method, fig.cap = "The origins of the data in the BIO-MATE data streams.", echo = FALSE, message = FALSE, warning=FALSE}
knitr::include_graphics("Method.png")
```

```{r userguide, fig.cap = "A schematic demonstrating the BIO-MATE workflow and how it is implemented in BIO-SHARE", echo = FALSE,message = FALSE, warning=FALSE}
knitr::include_graphics("BIOMATE_usage.png")
```

```{r spatiotemporaldist, fig.cap = "The spatiotemporal distribution of different data streams and bio-physical matches in the BIOMATE data compilation", echo = FALSE, message = FALSE, warning=FALSE, dpi = 700, fig.width = 8, fig.height = 6 }

### Pigment stream ####
loc_data = pig_data %>% filter(!is.na(DATE_analyser), !is.na(LAT_analyser))
loc_data$MM_p = loc_data$MM + 6
loc_data$MM_p[which(!is.na(loc_data$MM_p) & loc_data$MM_p > 12)] = loc_data$MM_p[which(!is.na(loc_data$MM_p) & loc_data$MM_p > 12)] - 12

points = loc_data[,c("LON_analyser","LAT_analyser")]
colnames(points) = c("long","lat")
coordinates(points) <- ~long+lat #check right way round
projection(points) <- "+proj=longlat +datum=WGS84"

extent_p1 = data.frame("x"=c(-180,90,180,-90), "y" = c(90,90,-90,-90))
coordinates(extent_p1) <- ~x+y
projection(extent_p1) <- "+proj=longlat +datum=WGS84"

rast <- raster(x =extent(extent_p1), nrows = 180/2, ncols = 360/2)
gridded_counts_profile = rasterize(points, rast, field = 1, fun = "count", background = 0)
gridded_counts_profile <- as(gridded_counts_profile, "SpatialPointsDataFrame")
gridded_counts_profile <-as.data.frame(gridded_counts_profile)
# background to NA
gridded_counts_profile$layer[which(gridded_counts_profile$layer == 0)] = NA


map_data= map_data("world")

density_all_pig =  ggplot() +
  #geom_raster(data = bathy2, aes(x = x, y=y, fill = z)) +
  geom_polygon(data =map_data, aes(x = long, y = lat, group = group),fill ="#a6611a", colour ="#a6611a") +
  xlab("") + 
  ylab("") +
 # geom_point(data = loc_data, aes(x=LON_analyser, y=LAT_analyser), col = "#a6611a", cex = 3) +
  # Adds axes
  # Change theme to remove axes and ticks
  theme(panel.background = element_blank(),legend.position = "none",
        panel.grid.minor = element_blank(),axis.ticks=element_blank(), axis.text = element_blank())+

  geom_raster(data = gridded_counts_profile, aes(x=x, y=y, fill = layer)) +
  scale_fill_gradient(name = "no. records",trans = "log",low = "#c7eae5", high = "#01665e",  na.value = "#FFFFFF00") +
  theme(legend.position = "right", legend.text = element_text(size = 6), legend.title = element_text(size = 6), legend.key.size = unit(0.5,"lines"))+ coord_fixed(ratio=1) #+
  # stat_contour(data = bathySOdf,aes(x=x,y=y,z=layer),breaks = c(-1000,0), col = "black") +
  # geom_path(data =fronts,aes(x=long,y=lat,group=group),col = "#8c510a") + geom_point(data = data.frame(points_ster), aes(x = long, y = lat))



# annual observation plot (include depth distribution, split into HPLC and fluor)
ann_obs = ggplot() + geom_bar(data = loc_data, aes(x = as.factor(YYYY)), fill = "grey69")+
  scale_x_discrete(name = "Year",breaks = seq(min(loc_data$YYYY, na.rm = T), max(loc_data$YYYY, na.rm = T),by = 5), labels = seq(min(loc_data$YYYY, na.rm = T), max(loc_data$YYYY, na.rm = T), 5)) + scale_y_continuous(name = "Number of records")+
  theme(axis.line.y.right  = element_line(color="grey69", size = 1),
        axis.line.x = element_line(color="black", size = 1),axis.line.y.left = element_line(color="black", size = 1), axis.ticks.y.right = element_line(colour = "grey69")
        ,panel.background = element_blank(),
        panel.grid.major = element_blank(),plot.title = element_text( size=6), axis.title.x =  element_text(size = 6),  axis.text.x = element_text(size = 6), 
        axis.text.y.left  = element_text(size = 6),axis.text.y.right = element_text(size = 6, colour = "grey69"), axis.title.y = element_text(size = 6))


# seasonal plot (include depth distribution, split into HPLC and fluor)
seas_obs = ggplot() + geom_bar(data = loc_data, aes(x = as.factor(MM_p)), fill = "grey69")+
  scale_x_discrete(name = "Month",breaks = c(1,6,12), labels = c("July", "Jan","June"))+ scale_y_continuous(name = "Number of records")+
  theme(axis.line.y.right  = element_line(color="grey69", size = 1),
        axis.line.x = element_line(color="black", size = 1),axis.line.y.left = element_line(color="black", size = 1), axis.ticks.y.right = element_line(colour = "grey69")
        ,panel.background = element_blank(),
        panel.grid.major = element_blank(),plot.title = element_text( size=6), axis.title.x =  element_text(size = 6),  axis.text.x = element_text(size = 6), 
        axis.text.y.left  = element_text(size = 6),axis.text.y.right = element_text(size = 6, colour = "grey69"), axis.title.y = element_text(size = 6))

# time of day
hour_obs = ggplot() + geom_histogram(data = loc_data %>% filter(!is.na(SA)), aes(x = SA), fill = "grey69", binwidth = 15)+
  scale_x_discrete(name = "Sun Angle",breaks = seq(-90,90,45), labels = seq(-90,90,45))+ scale_y_continuous(name = "Number of records")+
  theme(axis.line.y.right  = element_line(color="grey69", size = 1),
        axis.line.x = element_line(color="black", size = 1),axis.line.y.left = element_line(color="black", size = 1), axis.ticks.y.right = element_line(colour = "grey69")
        ,panel.background = element_blank(),
        panel.grid.major = element_blank(),plot.title = element_text( size=6), axis.title.x =  element_text(size = 6),  axis.text.x = element_text(size = 6), 
        axis.text.y.left  = element_text(size = 6),axis.text.y.right = element_text(size = 6, colour = "grey69"), axis.title.y = element_text(size = 6))

#depth
depth_obs = ggplot() + geom_histogram(data = loc_data %>% filter(!is.na(DEPTH)), aes(y = DEPTH), fill = "grey69", binwidth = 10)+ scale_y_reverse(limits = c(500,0))+ scale_x_continuous(position = "top", name = "Number of records", limits =c(0,10000))+
  theme(axis.line.y.right  = element_line(color="grey69", size = 1), 
        axis.line.x = element_line(color="black", size = 1),axis.line.y.left = element_line(color="black", size = 1), axis.ticks.y.right = element_line(colour = "grey69")
        ,panel.background = element_blank(),
        panel.grid.major = element_blank(),plot.title = element_text( size=6), axis.title.x =  element_text(size = 6),  axis.text.x = element_text(size = 6), 
        axis.text.y.left  = element_text(size = 6),axis.text.y.right = element_text(size = 6, colour = "grey69"), axis.title.y = element_text(size = 6))


#### Profiling Sensor stream ####
loc_CTD = CTD_df 
loc_CTD$LON[is.na(loc_CTD$LAT)] = loc_CTD$LON_e[is.na(loc_CTD$LAT)]
loc_CTD$LAT[is.na(loc_CTD$LAT)] = loc_CTD$LON_e[is.na(loc_CTD$LAT)]
loc_CTD$TIME_s[is.na(loc_CTD$LAT)] = loc_CTD$TIME_e[is.na(loc_CTD$LAT)]
loc_CTD = loc_CTD %>% filter(!is.na(LON), !is.na(YYYY)) %>% mutate(HH = as.numeric(substr(TIME_s, 12,13)))
loc_CTD$MM_p = loc_CTD$MM + 6
loc_CTD$MM_p[which(!is.na(loc_CTD$MM_p) & loc_CTD$MM_p > 12)] = loc_CTD$MM_p[which(!is.na(loc_CTD$MM_p) & loc_CTD$MM_p > 12)] - 12
loc_CTD$SA = unlist(lapply(1:nrow(loc_CTD),FUN = function(x){
  if(!is.na(loc_CTD$DATE[x]) & !is.na(loc_CTD$TIME_s[x]) & !is.na(loc_CTD$LAT[x]) & !is.na(loc_CTD$LON[x])){SA = sunAngle( t = as.POSIXct(paste(loc_CTD$DATE[x], loc_CTD$TIME_s[x]), tz = "UTC"), longitude = loc_CTD$LON[x], latitude = loc_CTD$LAT[x])$altitude}else{SA = NA}
  SA
}))


points = loc_CTD[,c("LON","LAT")]
colnames(points) = c("long","lat")
coordinates(points) <- ~long+lat #check right way round
projection(points) <- "+proj=longlat +datum=WGS84"

extent_p1 = data.frame("x"=c(-180,90,180,-90), "y" = c(90,90,-90,-90))
coordinates(extent_p1) <- ~x+y
projection(extent_p1) <- "+proj=longlat +datum=WGS84"

rast <- raster(x =extent(extent_p1), nrows = 180/2, ncols = 360/2)
gridded_counts_profile = rasterize(points, rast, field = 1, fun = "count", background = 0)
gridded_counts_profile <- as(gridded_counts_profile, "SpatialPointsDataFrame")
gridded_counts_profile <-as.data.frame(gridded_counts_profile)
# background to NA
gridded_counts_profile$layer[which(gridded_counts_profile$layer == 0)] = NA


map_data= map_data("world")

density_all_CTD =  ggplot() +
  #geom_raster(data = bathy2, aes(x = x, y=y, fill = z)) +
  geom_polygon(data =map_data, aes(x = long, y = lat, group = group),fill ="#a6611a", colour ="#a6611a") +
  xlab("") + 
  ylab("") +
 # geom_point(data = loc_data, aes(x=LON_analyser, y=LAT_analyser), col = "#a6611a", cex = 3) +
  # Adds axes
  # Change theme to remove axes and ticks
  theme(panel.background = element_blank(),legend.position = "none",
        panel.grid.minor = element_blank(),axis.ticks=element_blank(), axis.text = element_blank())+

  geom_raster(data = gridded_counts_profile, aes(x=x, y=y, fill = layer)) +
  scale_fill_gradient(name = "no. profiles",trans = "log",low = "#c7eae5", high = "#01665e",  na.value = "#FFFFFF00") +
  theme(legend.position = "right", legend.text = element_text(size = 6), legend.title = element_text(size = 6), legend.key.size = unit(0.5,"lines"))+ coord_fixed(ratio=1) #+
  # stat_contour(data = bathySOdf,aes(x=x,y=y,z=layer),breaks = c(-1000,0), col = "black") +
  # geom_path(data =fronts,aes(x=long,y=lat,group=group),col = "#8c510a") + geom_point(data = data.frame(points_ster), aes(x = long, y = lat))

# annual observation plot (include depth distribution, split into HPLC and fluor)
ann_obs_CTD = ggplot() + geom_bar(data = loc_CTD, aes(x = as.factor(YYYY)), fill = "grey69")+
  scale_x_discrete(name = "Year",breaks = seq(min(loc_CTD$YYYY, na.rm = T), max(loc_CTD$YYYY, na.rm = T),by = 5), labels = seq(min(loc_CTD$YYYY, na.rm = T), max(loc_CTD$YYYY, na.rm = T), 5)) + scale_y_continuous(name = "Number of profiles")+
  theme(axis.line.y.right  = element_line(color="grey69", size = 1),
        axis.line.x = element_line(color="black", size = 1),axis.line.y.left = element_line(color="black", size = 1), axis.ticks.y.right = element_line(colour = "grey69")
        ,panel.background = element_blank(),
        panel.grid.major = element_blank(),plot.title = element_text( size=6), axis.title.x =  element_text(size = 6),  axis.text.x = element_text(size = 6), 
        axis.text.y.left  = element_text(size = 6),axis.text.y.right = element_text(size = 6, colour = "grey69"), axis.title.y = element_text(size = 6))


# seasonal plot (include depth distribution, split into HPLC and fluor)
seas_obs_CTD = ggplot() + geom_bar(data = loc_CTD, aes(x = as.factor(MM_p)), fill = "grey69")+
  scale_x_discrete(name = "Month",breaks = c(1,6,12), labels = c("July", "Jan","June"))+ scale_y_continuous(name = "Number of profiles")+
  theme(axis.line.y.right  = element_line(color="grey69", size = 1),
        axis.line.x = element_line(color="black", size = 1),axis.line.y.left = element_line(color="black", size = 1), axis.ticks.y.right = element_line(colour = "grey69")
        ,panel.background = element_blank(),
        panel.grid.major = element_blank(),plot.title = element_text( size=6), axis.title.x =  element_text(size = 6),  axis.text.x = element_text(size = 6), 
        axis.text.y.left  = element_text(size = 6),axis.text.y.right = element_text(size = 6, colour = "grey69"), axis.title.y = element_text(size = 6))

# time of day
hour_obs_CTD = ggplot() + geom_histogram(data = loc_CTD %>% filter(!is.na(SA)), aes(x = SA), fill = "grey69", binwidth = 15)+
  scale_x_discrete(name = "Sun Angle",breaks = seq(-90,90,45), labels = seq(-90,90,45))+ scale_y_continuous(name = "Number of profiles")+
  theme(axis.line.y.right  = element_line(color="grey69", size = 1),
        axis.line.x = element_line(color="black", size = 1),axis.line.y.left = element_line(color="black", size = 1), axis.ticks.y.right = element_line(colour = "grey69")
        ,panel.background = element_blank(),
        panel.grid.major = element_blank(),plot.title = element_text( size=6), axis.title.x =  element_text(size = 6),  axis.text.x = element_text(size = 6), 
        axis.text.y.left  = element_text(size = 6),axis.text.y.right = element_text(size = 6, colour = "grey69"), axis.title.y = element_text(size = 6))

#depth
depth_CTD = ggplot() + geom_histogram(data = loc_CTD, aes(y = BOT_DEPTH), fill = "grey69", binwidth = 10)+ scale_y_reverse()+ scale_x_continuous(position = "top", name = "Number of profiles")+
  theme(axis.line.y.right  = element_line(color="grey69", size = 1), 
        axis.line.x = element_line(color="black", size = 1),axis.line.y.left = element_line(color="black", size = 1), axis.ticks.y.right = element_line(colour = "grey69")
        ,panel.background = element_blank(),
        panel.grid.major = element_blank(),plot.title = element_text( size=6), axis.title.x =  element_text(size = 6),  axis.text.x = element_text(size = 6), 
        axis.text.y.left  = element_text(size = 6),axis.text.y.right = element_text(size = 6, colour = "grey69"), axis.title.y = element_text(size = 6))


##### Biophysical matches #### 
match_data = pig_data %>% filter(!is.na(CTD_IDs))
loc_match = match_data %>% filter(!is.na(DATE_analyser), !is.na(LAT_analyser))
loc_match$MM_p = loc_match$MM + 6
loc_match$MM_p[which(!is.na(loc_match$MM_p) & loc_match$MM_p > 12)] = loc_match$MM_p[which(!is.na(loc_match$MM_p) & loc_match$MM_p > 12)] - 12

points = loc_match[,c("LON_analyser","LAT_analyser")]
colnames(points) = c("long","lat")
coordinates(points) <- ~long+lat #check right way round
projection(points) <- "+proj=longlat +datum=WGS84"

extent_p1 = data.frame("x"=c(-180,90,180,-90), "y" = c(90,90,-90,-90))
coordinates(extent_p1) <- ~x+y
projection(extent_p1) <- "+proj=longlat +datum=WGS84"

rast <- raster(x =extent(extent_p1), nrows = 180/2, ncols = 360/2)
gridded_counts_profile = rasterize(points, rast, field = 1, fun = "count", background = 0)
gridded_counts_profile <- as(gridded_counts_profile, "SpatialPointsDataFrame")
gridded_counts_profile <-as.data.frame(gridded_counts_profile)
# background to NA
gridded_counts_profile$layer[which(gridded_counts_profile$layer == 0)] = NA

map_data= map_data("world")

density_all_match =  ggplot() +
  #geom_raster(data = bathy2, aes(x = x, y=y, fill = z)) +
  geom_polygon(data =map_data, aes(x = long, y = lat, group = group),fill ="#a6611a", colour ="#a6611a") +
  xlab("") + 
  ylab("") +
 # geom_point(data = loc_match, aes(x=LON_analyser, y=LAT_analyser), col = "#a6611a", cex = 3) +
  # Adds axes
  # Change theme to remove axes and ticks
  theme(panel.background = element_blank(),legend.position = "none",
        panel.grid.minor = element_blank(),axis.ticks=element_blank(), axis.text = element_blank())+

  geom_raster(data = gridded_counts_profile, aes(x=x, y=y, fill = layer)) +
  scale_fill_gradient(name = "no. records",trans = "log",low = "#c7eae5", high = "#01665e",  na.value = "#FFFFFF00") +
  theme(legend.position = "right", legend.text = element_text(size = 6), legend.title = element_text(size = 6), legend.key.size = unit(0.5,"lines"))+ coord_fixed(ratio=1) #+
  # stat_contour(data = bathySOdf,aes(x=x,y=y,z=layer),breaks = c(-1000,0), col = "black") +
  # geom_path(data =fronts,aes(x=long,y=lat,group=group),col = "#8c510a") + geom_point(data = data.frame(points_ster), aes(x = long, y = lat))



# annual observation plot (include depth distribution, split into HPLC and fluor)
ann_obs_match = ggplot() + geom_bar(data = loc_match, aes(x = as.factor(YYYY)), fill = "grey69")+
  scale_x_discrete(name = "Year",breaks = seq(min(loc_match$YYYY, na.rm = T), max(loc_match$YYYY, na.rm = T),by = 5), labels = seq(min(loc_match$YYYY, na.rm = T), max(loc_match$YYYY, na.rm = T), 5)) + scale_y_continuous(name = "Number of records")+
  theme(axis.line.y.right  = element_line(color="grey69", size = 1),
        axis.line.x = element_line(color="black", size = 1),axis.line.y.left = element_line(color="black", size = 1), axis.ticks.y.right = element_line(colour = "grey69")
        ,panel.background = element_blank(),
        panel.grid.major = element_blank(),plot.title = element_text( size=6), axis.title.x =  element_text(size = 6),  axis.text.x = element_text(size = 6), 
        axis.text.y.left  = element_text(size = 6),axis.text.y.right = element_text(size = 6, colour = "grey69"), axis.title.y = element_text(size = 6))


# seasonal plot (include depth distribution, split into HPLC and fluor)
seas_obs_match = ggplot() + geom_bar(data = loc_match, aes(x = as.factor(MM_p)), fill = "grey69")+
  scale_x_discrete(name = "Month",breaks = c(1,6,12), labels = c("July", "Jan","June"))+ scale_y_continuous(name = "Number of records")+
  theme(axis.line.y.right  = element_line(color="grey69", size = 1),
        axis.line.x = element_line(color="black", size = 1),axis.line.y.left = element_line(color="black", size = 1), axis.ticks.y.right = element_line(colour = "grey69")
        ,panel.background = element_blank(),
        panel.grid.major = element_blank(),plot.title = element_text( size=6), axis.title.x =  element_text(size = 6),  axis.text.x = element_text(size = 6), 
        axis.text.y.left  = element_text(size = 6),axis.text.y.right = element_text(size = 6, colour = "grey69"), axis.title.y = element_text(size = 6))

# time of day
hour_obs_match = ggplot() + geom_bar(data = loc_match %>% filter(!is.na(SA)), aes(x = SA), fill = "grey69", binwidth = 15)+
  scale_x_discrete(name = "Sun Angle",breaks = seq(-90,90,45), labels = seq(-90,90,45))+ scale_y_continuous(name = "Number of records")+
  theme(axis.line.y.right  = element_line(color="grey69", size = 1),
        axis.line.x = element_line(color="black", size = 1),axis.line.y.left = element_line(color="black", size = 1), axis.ticks.y.right = element_line(colour = "grey69")
        ,panel.background = element_blank(),
        panel.grid.major = element_blank(),plot.title = element_text( size=6), axis.title.x =  element_text(size = 6),  axis.text.x = element_text(size = 6), 
        axis.text.y.left  = element_text(size = 6),axis.text.y.right = element_text(size = 6, colour = "grey69"), axis.title.y = element_text(size = 6))

#depth
depth_obs_match = ggplot() + geom_histogram(data = loc_match %>% filter(!is.na(DEPTH)), aes(y = DEPTH), fill = "grey69", binwidth = 10)+ scale_y_reverse(limits = c(500,0))+ scale_x_continuous(position = "top", name = "Number of records", limits =c(0,10000))+
  theme(axis.line.y.right  = element_line(color="grey69", size = 1), 
        axis.line.x = element_line(color="black", size = 1),axis.line.y.left = element_line(color="black", size = 1), axis.ticks.y.right = element_line(colour = "grey69")
        ,panel.background = element_blank(),
        panel.grid.major = element_blank(),plot.title = element_text( size=6), axis.title.x =  element_text(size = 6),  axis.text.x = element_text(size = 6), 
        axis.text.y.left  = element_text(size = 6),axis.text.y.right = element_text(size = 6, colour = "grey69"), axis.title.y = element_text(size = 6))

lay = rbind(c(1,1,2,2,5),
            c(1,1,2,2,5),
            c(1,1,3,4,5),
            c(1,1,3,4,5),
            c(6,6,7,7,10),
            c(6,6,7,7,10),
            c(6,6,8,9,10),
            c(6,6,8,9,10),
            c(11,11,12,12,15),
            c(11,11,12,12,15),
            c(11,11,13,14,15),
            c(11,11,13,14,15))

grid.arrange(density_all_pig, ann_obs, seas_obs, hour_obs, depth_obs, density_all_CTD, ann_obs_CTD, seas_obs_CTD, hour_obs_CTD, depth_CTD, density_all_match, ann_obs_match, seas_obs_match, hour_obs_match, depth_obs_match, layout_matrix = lay)


```


```{r physvariabledist, fig.cap = "The distribution of temperature and salinity data measured at 10m by profiling sensors in the BIOMATE data compilation", echo = FALSE, message = FALSE, warning=FALSE, fig.width= 8, fig.height= 10}
lat_data = data.frame(lat = CTD_df$LAT, sal = CTD_df$CTDSAL_10, temp = (CTD_df$CTDTEMP_10+50)/2)
lat_data = melt(lat_data, id.var = "lat")

sal_hist = ggplot()+ geom_histogram(data = CTD_df, aes(x = CTDSAL_10), binwidth = 0.1)+ xlim(c(25,40)) +xlab("Salinity at 10 m") + ylab("Number of records")
temp_hist = ggplot()+ geom_histogram(data = CTD_df, aes(x = CTDTEMP_10), binwidth = 0.5) + xlab("Temperature at 10 m (\u00B0C)") + ylab("Number of records")

lat_plot = ggplot() + geom_point(data = lat_data, aes(x = lat, y = value, colour = variable)) + scale_colour_manual(values = c("black","blue"))+ theme(legend.position = "none") + xlab("Latitude  (\u00B0N)") + scale_y_continuous(name = "Salinity at 10 m",limits =c(25, 40),  sec.axis = sec_axis(~(. - 25)*2, name = "Temperature at 10 m (\u00B0C)"))  + theme(axis.title.y.right = element_text(color = "blue"))

grid.arrange(sal_hist, temp_hist, lat_plot, layout_matrix = rbind(c(1,1), c(2,2), c(3,3)))
```

```{r chldist, fig.cap = "The (a) distribution of Chla, TCHLA and FCHLA in the BIO-MATE data compilation and (b) the location of high (>10 \u03BCg/l) Chla, TCHLA and FCHLA measurements.", echo = FALSE, message = FALSE, warning=FALSE, fig.width = 5, fig.height = 5}

# variable histograms (chla, fchla, temp, sal)
chl_data = data.frame(TCHLA = pig_data$TCHLA, Chla = pig_data$Chla, FChla = pig_data$FCHLORA)
chl_data = melt(chl_data) %>% filter(!is.na(value))
# chl_data = chl_data[-which(chl_data$value == -999),]
# chl_data = chl_data[-which(chl_data$value == -888),]
chl_hist = ggplot()+ geom_histogram(data = chl_data, aes(x = value, fill = variable), binwidth = 0.2) + xlim(c(0,10)) + ylab("Number of Records") + scale_fill_discrete(labels = c("HPLC derrived Total Chlorophyll", "HPLC derrived Total Chlorophyll-a", "Fluorometrically derrived Chlorophyl")) + xlab("Concentration (\u03BCg/L)") + theme(legend.position = "top", legend.text = element_text(size = 8))

#plot high chlorophyll values
points = loc_data %>% filter(FCHLORA > 10 | Chla > 10 | TCHLA > 10)
points = points[,c("LON_analyser","LAT_analyser")]
colnames(points) = c("long","lat")
coordinates(points) <- ~long+lat #check right way round
projection(points) <- "+proj=longlat +datum=WGS84"

extent_p1 = data.frame("x"=c(-180,90,180,-90), "y" = c(90,90,-90,-90))
coordinates(extent_p1) <- ~x+y
projection(extent_p1) <- "+proj=longlat +datum=WGS84"

rast <- raster(x =extent(extent_p1), nrows = 180/2, ncols = 360/2)
gridded_counts_profile = rasterize(points, rast, field = 1, fun = "count", background = 0)
gridded_counts_profile <- as(gridded_counts_profile, "SpatialPointsDataFrame")
gridded_counts_profile <-as.data.frame(gridded_counts_profile)
# background to NA
gridded_counts_profile$layer[which(gridded_counts_profile$layer == 0)] = NA


map_data= map_data("world")

density_all_high =  ggplot() +
  #geom_raster(data = bathy2, aes(x = x, y=y, fill = z)) +
  geom_polygon(data =map_data, aes(x = long, y = lat, group = group),fill ="#a6611a", colour ="#a6611a") +
  xlab("") + 
  ylab("") +
 # geom_point(data = loc_data, aes(x=LON_analyser, y=LAT_analyser), col = "#a6611a", cex = 3) +
  # Adds axes
  # Change theme to remove axes and ticks
  theme(panel.background = element_blank(),legend.position = "none",
        panel.grid.minor = element_blank(),axis.ticks=element_blank(), axis.text = element_blank())+

  geom_raster(data = gridded_counts_profile, aes(x=x, y=y, fill = layer)) +
  scale_fill_gradient(name = "no. records",trans = "log",low = "#c7eae5", high = "#01665e",  na.value = "#FFFFFF00") +
  theme(legend.position = "right", legend.key.size = unit(0.5,"lines"), legend.text = element_text(size = 6), legend.title = element_blank())+ coord_fixed(ratio=1) #+
  # stat_contour(data = bathySOdf,aes(x=x,y=y,z=layer),breaks = c(-1000,0), col = "black") +
  # geom_path(data =fronts,aes(x=long,y=lat,group=group),col = "#8c510a") + geom_point(data = data.frame(points_ster), aes(x = long, y = lat))

grid.arrange(chl_hist, density_all_high, layout_matrix = rbind(c(1),c(2)))
```

```{r CTDdist, fig.cap = "The time difference between the bottom and end possitions of a profiling sensor cast versus the bottom depth of the cast.", echo = FALSE, message = FALSE, warning=FALSE}

#CTD cast times vs depth
#If you do a regression, rmeove BOT_depth <150 and T_diff_e >60 000
# can predict bot time (95% RSE interval ) to 20 mins. data from 34 voyages, 4 ships
# can predict end time to 1:10 h. data from 38 voyages, 6 ships

CTD_diff_df = melt(CTD_df[,c("BOT_DEPTH", "T_diff_e", "T_diff_b")], id.vars = "BOT_DEPTH", value = "value") %>% filter(!is.na(value))
CTD_diff_df$value = CTD_diff_df$value/(60*60)

ggplot()+ geom_point(data = CTD_diff_df, aes(x = BOT_DEPTH, y = value, colour = variable)) +ylim(c(0,10)) +scale_colour_manual(values = c("#a6611a", "#01665e"), labels = c("Start to bottom position", "Start to end position")) + xlab("Bottom depth (meters)") + ylab("Time difference (hours)")+
   theme(axis.line.y.right  = element_line(color="grey69", size = 1),
        axis.line.x = element_line(color="black", size = 1),axis.line.y.left = element_line(color="black", size = 1), axis.ticks.y.right = element_line(colour = "grey69")
        ,panel.background = element_blank(),legend.position = "top",
        panel.grid.major = element_blank(),plot.title = element_text( size=12), axis.title.x =  element_text(size = 12),  axis.text.x = element_text(size = 10), 
        axis.text.y.left  = element_text(size = 10),axis.text.y.right = element_text(size = 12, colour = "grey69"), axis.title.y = element_text(size = 12),
        plot.margin = unit(c(1,3.5,1,1), "lines"), legend.title = element_blank()) 

```

```{r hplcvsfluor, fig.cap = "A comparison of fluorometrically derived chlorophyll (Chl) methods against total chlorophyll-a (TChla) derived from HPLC measurments", echo = FALSE, message = FALSE, warning=FALSE}
# HPLC vs fluor
# maybe look at bathymetry in PALMER
method_info = BIOMATE::method_info
B = data.frame("unique" = paste(pig_data$EXPOCODE, pig_data$DATE, pig_data$TIME_s, pig_data$TIME_b, pig_data$TIME_e, pig_data$DATE_analyser, pig_data$CTD_IDs, round(pig_data$LAT_analyser, digits = 1), round(pig_data$LON_analyser, digits = 1), round(pig_data$DEPTH, digits = 0), pig_data$STNNBR_analyser), "method" = pig_data$PIG_METHOD)
C = paste(B$unique, B$method)
B = B[!duplicated(C),]
n_method = stats::aggregate(B$method, by = list(B$unique), FUN = length)

pig_compare  = data.frame("unique" = paste(pig_data$EXPOCODE, pig_data$DATE, pig_data$TIME_s, pig_data$TIME_b, pig_data$TIME_e, pig_data$DATE_analyser, pig_data$CTD_IDs, round(pig_data$LAT_analyser, digits = 1), round(pig_data$LON_analyser, digits = 1), round(pig_data$DEPTH, digits = 0), pig_data$STNNBR_analyser), "hplc" = pig_data$TCHLA, "fluor" = pig_data$FCHLORA)
#pig_compare = pig_compare[-which(pig_data$Chla == -999),]
#pig_compare = pig_compare[-which(pig_data$Chla == -2997),]
tchla_calc = rowSums(pig_data[,c("Chla","Chla_ide","Chla_allom", "Chla_prime", "DVChla")], na.rm = T)
tchla_calc[rowSums(is.na(pig_data[,c("Chla","Chla_ide","Chla_allom", "Chla_prime", "DVChla")])) == 5] = NA
# tchla_calc = tchla_calc[-which(pig_data$Chla == -999)]
pig_compare$hplc[is.na(pig_compare$hplc)] = tchla_calc[is.na(pig_compare$hplc)]
pig_compare = pig_compare %>% filter(!is.na(hplc) | !is.na(fluor), unique %in% n_method$Group.1[which(n_method$x == 2)])
  
pig_compare = melt(pig_compare, id.vars = "unique", value = "value") %>% filter(!is.na(value))
pig_compare= left_join(pig_compare[which(pig_compare$variable == "hplc"),c("unique","value")], pig_compare[which(pig_compare$variable == "fluor"),c("unique","value")], by = c("unique"))
pig_compare = pig_compare %>% filter(!is.na(value.y))
fluor_only = B[grepl(pattern = "Bench-top Fluorometry", B$method),]
fluor_only$method = as.character(fluor_only$method)
pig_compare = left_join(pig_compare, fluor_only, by = c("unique"))

for(mt in 1:nrow(method_info)){
  pig_compare$method[which(grepl(pattern = method_info$Method[mt],  pig_compare$method))]= method_info$Method[mt]
}

# pig_compare_plot = ggplot() + geom_point(data = pig_compare, aes(x = value.x, y = value.y))

# get_density <- function(x, y, ...) {
#   dens <- MASS::kde2d(x, y, ...)
#   ix <- findInterval(x, dens$x)
#   iy <- findInterval(y, dens$y)
#   ii <- cbind(ix, iy)
#   return(dens$z[ii])
# }

#pig_compare$density <- get_density(pig_compare$value.x, pig_compare$value.y, n = 5000)
pig_compare = pig_compare %>% filter(!method %in% c("Mueller_2003", "Unknown_fluorometry"))
ggplot() + geom_point(data = pig_compare, aes(x = value.x, y = value.y, colour = method)) + facet_grid(vars(method)) + xlim(c(0,30)) + ylim(c(0,30)) + xlab("TCHLA derrived from HPLC") + ylab("Chl derrived fluorometrically")

```
# Tables

```{r processingmetadata, tab.cap = "a figure", echo = FALSE, message = FALSE, warning=FALSE}
p_meta = data.frame("variable"= character(), "stream" = character(), "description" = character(), "input"= character())
p_meta = p_meta %>% 
  add_row(variable = "file_type", stream = "all", description = "The format of the file/s.", input = "text delim or netcdf") %>%   
  add_row(variable = "path", stream = "all", description = "A path to where the file/s is stored.", input = "A pathname that is R compatible") %>%   
  add_row(variable = "extention", stream = "all", description = "The extension of the file/s.", input = "text delim or netcdf") %>%   
  add_row(variable = "delim", stream = "all", description = "Only fill if rectangular text-delimited file/s.", input = "rect") %>%  
  add_row(variable = "header_sep", stream = "all", description = "A separator used in headers of the file. Headers often store location data in profiling datasets and need extraction. Can be left empty.", input = "colon, comma, dash, equals, space") %>%  
  add_row(variable = "missing_value", stream = "all", description = "The value or character used to indicate a missing value.", input = "value") %>%  
  add_row(variable = "not_detected", stream = "PIG, POC", description = "The value or character used to indicate a variable was not detected in analysis.", input = "value") %>%  
  
  add_row(variable = "EXPOCODE", stream = "all", description = "The EXPOCODE of the voyage associated with the data.", input = "12-digit code") %>%
  add_row(variable = "source", stream = "all", description = "The data repository the data files were sourced from.", input = "The short name of the data repository used within BIO-MATE. See https://github.com/KimBaldry/BIO-MATE/product_data/supporting_information/BIOMATE_SOURCES.txt.") %>%
  add_row(variable = "PI", stream = "all", description = "The principle investigator/s responsible for the published dataset.", input = "Names sepparated by a dash") %>%
    add_row(variable = "Institution", stream = "all", description = "The institution/s who collected the data.", input = "Names sepparated by a dash") %>%
  add_row(variable = "contact", stream = "all", description = "A contact for the published dataset.", input = "E-mail address") %>%
  add_row(variable = "citation", stream = "all", description = "The BIO-MATE citation tag/s used to reference a BibTEX entry for the published dataset.", input = "A BIO-MATE citation tag") %>%
  add_row(variable = "analysis_type", stream = "PIG, POC", description = "The type of analysis used on water samples for the published dataset.", input = "A code to reference an analysis type. See https://github.com/KimBaldry/BIO-MATE/product_data/supporting_information/BIOMATE_METHODS.txt") %>%
  add_row(variable = "Method", stream = "PIG, POC", description = "The method used to analyse water samples for the published dataset.", input = "A code to reference a method. See https://github.com/KimBaldry/BIO-MATE/product_data/supporting_information/BIOMATE_METHODS.txt") %>%
  
  add_row(variable = "TZ", stream = "all", description = "The time zone for date and time information.", input = "Time zone code") %>%
  add_row(variable = "STNNBR", stream = "all", description = "The name of the variable for the station number of the profiling staation.", input = "Text. If recorded in header use header-[variable].") %>%
  add_row(variable = "CASTNO", stream = "all", description = "The name of the variable for the cast number at the profiling station.", input = "Text. If recorded in header use header-[variable].") %>%
  add_row(variable = "DATE", stream = "PROF", description = "The name of the variable for the date of the profiling cast.", input = "Text. If recorded in header use header-[variable].") %>%
  add_row(variable = "DATE_analyser", stream = "PIG, POC", description = "The name of the variable for date of observation recorded by the analyser.", input = "Text. If recorded in header use header-[variable].") %>%
  add_row(variable = "DATE_format", stream = "PROF", description = "Format for DATE.", input = "A format string code. See strptime in R for codes.") %>%
  add_row(variable = "DATE_analyser_format", stream = "PIG, POC", description = "Format for DATE_analyser.", input = "A format string code. See strptime in R for codes.") %>%
  add_row(variable = "TIME_s", stream = "PROF", description = "The name of the variable for time at the start of teh profiling cast.", input = "Text. If recorded in header use header-[variable].") %>%
  add_row(variable = "TIME_b", stream = "PROF", description = "The name of the variable for time at the bottom of the profiling cast.", input = "Text. If recorded in header use header-[variable].") %>%
  add_row(variable = "TIME_e", stream = "PROF", description = "The name of the variable for time at the end of the profiling cast.", input = "Text. If recorded in header use header-[variable].") %>%
  add_row(variable = "TIME_analyser", stream = "PIG, POC", description = "The name of the variable for time of observation recorded by the analyser.  If recorded in header use header-[variable].", input = "Text") %>%
  add_row(variable = "TIME_format", stream = "PROF", description = "Format for TIME.", input = "A format string code. See strptime in R for codes.") %>%
  add_row(variable = "TIME_b_format", stream = "PROF", description = "Format for TIME_b, if different to TIME_format.", input = "A format string code. See strptime in R for codes.") %>%
  add_row(variable = "TIME_analyser_format", stream = "PIG, POC", description = "Format of TIME_analyser.", input = "A format string code. See strptime in R for codes.") %>%

  add_row(variable = "LATITUDE_s", stream = "PROF", description = "The name of the variable for latitude at the start of the profiling cast.", input = "Text") %>%
  add_row(variable = "LATITUDE_b", stream = "PROF", description = "The name of the variable for latitude at the bottom of the profiling cast.", input = "Text") %>%
  add_row(variable = "LATITUDE_e", stream = "PROF", description = "The name of the variable for latitude at the end of the profiling cast.", input = "Text") %>%
  add_row(variable = "LONGITUDE_s", stream = "PROF", description = "The name of the variable for longitude at the start of the profiling cast.", input = "Text") %>%
  add_row(variable = "LONGITUDE_b", stream = "PROF", description = "The name of the variable for longitude at the bottom of the profiling cast.", input = "Text") %>%
  add_row(variable = "LONGITUDE_e", stream = "PROF", description = "The name of the variable for longitude at the end of the profiling cast.", input = "Text") %>%
  add_row(variable = "LAT_analyser", stream = "PIG, POC", description = "The name of the variable for latitude recorded by the analyser.", input = "Text") %>%
  add_row(variable = "LON_analyser", stream = "PIG, POC", description = "The name of the variable for longitude recorded by the analyser.", input = "Text") %>%
  add_row(variable = "POSITION_format", stream = "all", description = "The format of latitude and longitude data.", input = "A string describing the format made up of %deg (degrees), %min (minutes), %sec (seconds) and %pos (for N/S/E/W specification)") %>%
  add_row(variable = "Sample_ID", stream = "PIG, POC", description = "The name of the variable containing sample identification.", input = "Text") %>%
  add_row(variable = "BOTTLE", stream = "PIG, POC", description = "The name of the variable containing bottle identifications.", input = "Text") %>%
  add_row(variable = "Underway_ID", stream = "PIG, POC", description = "How underway samples are identified within the dataset. Leave blank if there are no underway values within the dataset.", input = "[variable name]-[value] or all") %>%  
  
  add_row(variable = "CTDPRS", stream = "PROF", description = "The name of the variable for pressure collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDPRS_u", stream = "PROF", description = "The units for pressure collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDTMP", stream = "PROF", description = "The name of the variable for temperature collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDTMP_u", stream = "PROF", description = "The units for temperature collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDSAL", stream = "PROF", description = "The name of the variable for salinity collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDSAL_u", stream = "PROF", description = "The units for salinity collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDOXY", stream = "PROF", description = "The name of the variable for oxygen collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDOXY_u", stream = "PROF", description = "The units for oxygen collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDFLUOR", stream = "PROF", description = "The name of the variable for fluorescence collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDFLUOR_u", stream = "PROF", description = "The units for fluorescence collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDBEAMCP", stream = "PROF", description = "The name of the variable for beam attenuation collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDBEAMCP_u", stream = "PROF", description = "The units for beam attenuation collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDBBP700", stream = "PROF", description = "The name of the variable for optical backscatter (700 nm) collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDBBP700_u", stream = "PROF", description = "The units for optical backscatter (700 nm) collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDXMISS", stream = "PROF", description = "The name of the variable for transmittance collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDXMISS_u", stream = "PROF", description = "The units for transmittance collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDPAR", stream = "PROF", description = "The name of the variable for photosyntheitically active radiation collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDPAR_u", stream = "PROF", description = "The units for photosyntheitically active radiation collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDNITRATE", stream = "PROF", description = "The name of the variable for oxygen collected by the profiling sensor.", input = "Text") %>%
  add_row(variable = "CTDNITRATE_u", stream = "PROF", description = "The units for oxygen collected by the profiling sensor.", input = "Text") %>%
  
  add_row(variable = "DEPTH", stream = "PIG,POC", description = "The name of the variable for depth of observation recorded by the analyser.", input = "Text") %>%
  add_row(variable = "PIG_u", stream = "PIG", description = "The units for pigment measurements recorded by the analyser.", input = "Text") %>%
  add_row(variable = "FCHLORA", stream = "PIG", description = "The name of the variable for fluorometrically derived chlorophyll.", input = "Text") %>%
  add_row(variable = "FPHEO", stream = "PIG", description = "The name of the variable for fluorometrically derived phaeopigments.", input = "Text") %>%
  add_row(variable = "FPHYTIN", stream = "PIG", description = "The name of the variable for fluorometrically derived phaeophytin.", input = "Text") %>%
  add_row(variable = "TCHLA", stream = "PIG", description = "The name of the variable for HPLC derived total chlorophyll a.", input = "Text") %>%
  add_row(variable = "TACC", stream = "PIG", description = "The name of the variable for HPLC derived total accessory pigments.", input = "Text") %>%
  add_row(variable = "DVChla", stream = "PIG", description = "The name of the variable for HPLC derived divinyl chlorophyll a.", input = "Text") %>%
  add_row(variable = "Chla", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll a.", input = "Text") %>%
  add_row(variable = "Chla_ide", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyllide.", input = "Text") %>%
  add_row(variable = "Chla_allom", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll a allomers.", input = "Text") %>%
  add_row(variable = "Chla_prime", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll a prime.", input = "Text") %>%
  add_row(variable = "Chlb", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll b.", input = "Text") %>%
  add_row(variable = "DVChlb", stream = "PIG", description = "The name of the variable for HPLC derived divinyl chlorophyll b.", input = "Text") %>%
  add_row(variable = "Chlc", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll c.", input = "Text") %>%
  add_row(variable = "Chlc1_Chlc2_Mg_3_8_divinyl_pheoporphyrin_a5", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll c1 + chlorophyll c2 + Mg 3,8 divinyl pheoporphyrin a5.", input = "Text") %>%
  add_row(variable = "Chlc1", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll c1.", input = "Text") %>%
  add_row(variable = "Chlc1_like", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll c1-like.", input = "Text") %>%
  add_row(variable = "Chlc2", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll c2.", input = "Text") %>%
  add_row(variable = "Chlc1_Chlc2", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll c1 + chlorophyll c2.", input = "Text") %>%
  add_row(variable = "Chlc3", stream = "PIG", description = "The name of the variable for HPLC derived chlorophyll c3.", input = "Text") %>%
  add_row(variable = "MgDVP", stream = "PIG", description = "The name of the variable for HPLC derived Mg 2,4 divinyl pheoporphyrin a5 monomethyl ester.", input = "Text") %>%
    add_row(variable = "19Hex", stream = "PIG", description = "The name of the variable for HPLC derived 19’hexanoyloxyfucoxanthin.", input = "Text") %>%
  add_row(variable = "19But", stream = "PIG", description = "The name of the variable for HPLC derived 19’butanoyloxyfucoxanthin.", input = "Text") %>%
    add_row(variable = "Fucox", stream = "PIG", description = "The name of the variable for HPLC derived fucoxanthin.", input = "Text") %>%
  add_row(variable = "Perid", stream = "PIG", description = "The name of the variable for HPLC derived peridinin.", input = "Text") %>%
    add_row(variable = "Prasino", stream = "PIG", description = "The name of the variable for HPLC derived prasinoxanthin.", input = "Text") %>%
  add_row(variable = "Allox", stream = "PIG", description = "The name of the variable for HPLC derived alloxanthin.", input = "Text") %>%
    add_row(variable = "Lutein", stream = "PIG", description = "The name of the variable for HPLC derived lutein.", input = "Text") %>%
  add_row(variable = "Zeax", stream = "PIG", description = "The name of the variable for HPLC derived zeaxanthin.", input = "Text") %>%
    add_row(variable = "Zea_Lut", stream = "PIG", description = "The name of the variable for HPLC derived zeaxanthin + lutein.", input = "Text") %>%
  add_row(variable = "Violax", stream = "PIG", description = "The name of the variable for HPLC derived violaxanthin.", input = "Text") %>%
    add_row(variable = "Alpha_car", stream = "PIG", description = "The name of the variable for HPLC derived alpha carotene.", input = "Text") %>%
  add_row(variable = "Beta_car", stream = "PIG", description = "The name of the variable for HPLC derived beta carotene.", input = "Text") %>%
    add_row(variable = "Gamma_car", stream = "PIG", description = "The name of the variable for HPLC derived gamma carotene.", input = "Text") %>%
  add_row(variable = "Epsilon_car", stream = "PIG", description = "The name of the variable for HPLC derived epsilon carotene.", input = "Text") %>%
      add_row(variable = "Alpha_Beta_car", stream = "PIG", description = "The name of the variable for HPLC derived alpha + beta carotene.", input = "Text") %>%
  add_row(variable = "Neox", stream = "PIG", description = "The name of the variable for HPLC derived neoxanthin.", input = "Text") %>%
      add_row(variable = "DD", stream = "PIG", description = "The name of the variable for HPLC derived diadinoxanthin.", input = "Text") %>%
  add_row(variable = "DT", stream = "PIG", description = "The name of the variable for HPLC derived diatoxanthin.", input = "Text") %>%
      add_row(variable = "Viol_Neox", stream = "PIG", description = "The name of the variable for HPLC derived violaxanthin + neoxanthin.", input = "Text") %>%
  add_row(variable = "Phaeopigments", stream = "PIG", description = "The name of the variable for HPLC derived bulk phaeopigments.", input = "Text") %>%
        add_row(variable = "Phide_a", stream = "PIG", description = "The name of the variable for HPLC derived phaeophorbide a.", input = "Text") %>%
  add_row(variable = "Phytin_a", stream = "PIG", description = "The name of the variable for HPLC derived phaeophytin a.", input = "Text")
  
colnames(p_meta) = c("Processing metadata variable", "Data stream/s","Description", "Input Guide" )  

#knitr::kable(p_meta, row.names = F) %>%  pack_rows("File format information", 1,7)  %>% pack_rows("Data aquisition information", 8,15) %>% pack_rows("Location data information", 16,41) %>%  pack_rows("Profiling sensor data information", 42,61) %>% pack_rows("Pigment and POC data information", 62,105)
p_meta_adj = rbind(rep("File format information", 4),p_meta[1:7,], rep("Data aquisition information",4), p_meta[8:15,], rep("Location data information", 4), p_meta[16:41,], rep("Profiling sensor data information",4), p_meta[42:61,],rep("Pigment and POC data information",4), p_meta[62:105,])

p_meta_adj %>% flextable() %>% autofit() %>% merge_h() %>% bold(i = c(1, 9, 18, 45, 66)) %>% set_table_properties(layout = "autofit")

save(p_meta, file = "metadata_info.RData")
```

<br>
```{r datarecordsummary, tab.cap = "a figure", echo = FALSE, message = FALSE, warning=FALSE}
data_description= as.data.frame(t(data_description))
colnames(data_description) = c("All pigment records","Subsurface profile records (>4 depth samples above 75 m)","Surface records (<10 m)")

  
data_description$group =   c("Number of voyages", "Number of Records", "Unique samples (no replicates)", "Unique samples in lat and lon (not depth)", "Bio-physical matches", "HPLC and fluorometry matches", "Number of HPLC records", "Number of Fluorometry records", "Matches with CTDFLUOR records", "Matches with CTDBBP700 records", "Matches with CTDBEAMCP records")


data_description[,c(4,1:3)] %>%  flextable() %>% autofit() %>% set_table_properties(layout = "autofit")
#knitr::kable(data_description, row.names = F) %>% kable_paper("striped", full_width = F) %>% pack_rows("All pigment records", 1,1) %>% pack_rows("Subsurface profile records (>4 depth samples above 75 m)", 2, 2) %>% pack_rows("Surface records (<10 m)", 3,3) 


```
<br>

```{r CTD_des, echo = FALSE, message = FALSE, warning=FALSE}
CTD_des = as.data.frame(t(CTD_des))
CTD_des$group = c("Number of voyages", "Number of profiles", "Profiles with pressure records (%)", "Profiles with salinity records", "Profiles with temperature records", "Profiles with oxygen records", "Profiles with fluorescence records", "Profiles with beam attenuation records", "Profiles with backscatter 700nm records", "profiles with transmittance records", "Profiles with PAR records", "Profiles with nitrate records")

CTD_des %>% head() %>%  flextable() %>% autofit() %>% set_table_properties(layout = "autofit")
```

<br>

```{r profrecords, tab.cap = "a figure", echo = FALSE, message = FALSE, warning=FALSE}
prof_rec = data.frame("var" = as.character(), "des" = as.character())
# Header variables
prof_rec = prof_rec %>% 
  add_row(var = "ORIGINAL_CTDFILE/S", des = "The name of the original file/s") %>%
  add_row(var = "CTDFILE_MOD_DATE", des = "The modification date of the file") %>%
  add_row(var = "SOURCED_FROM", des = "The repository where data was originally sourced from") %>%
  add_row(var = "DATASET_CONTACT", des = "Name and email of the listed dataset contact") %>%
  add_row(var ="DOI/s", des = "Doi/s of original files") %>%
  add_row(var = "BIOMATE_CITE_TAGS", des = "The BIOMATE citation tags that are associated with the data, methods and source repository") %>%
  add_row(var = "DATA_CITATION/S", des = "The full citations associted with the data" ) %>%
  
  # Header variables
  add_row(var = "NUMBER_HEADERS", des = "The number of header variables") %>%
  add_row(var = "EXPOCODE", des = "The EXPOCODE associated with the data" ) %>% 
  add_row(var = "SHIP", des = "The vessel on which the data was collected") %>%
  add_row(var = "STNNBR or EVENTNBR", des = "The station number of the profiling station") %>%
  add_row(var = "CASTNO", des = "The cast number of the profiling station") %>% 
  add_row(var = "CTD_IDs", des = "An identifcation for the profiling station") %>% 
  add_row(var = "DATE", des = "The date of the profiling station") %>%
  add_row(var = "TIMEZONE", des = "The timezone the data was collected in") %>%
  add_row(var = "CTD_START_TIME", des = "The time at the start of the profiling station") %>%  
  add_row(var = "CTD_START_LATITUDE", des = "The latitude at the start of the profiling station") %>%  
  add_row(var = "CTD_START_LONGITUDE", des = "The longitude at the start of the profiling station") %>%  
  add_row(var = "CTD_BOTTOM_TIME", des = "The time at the bottom of the profiling station") %>%  
  add_row(var = "CTD_BOTTOM_LATITUDE", des = "The latitude at the bottom of the profiling station") %>% 
    add_row(var = "CTD_BOTTOM_LONGITUDE", des = "The longitude at the bottom of the profiling station") %>% 
  add_row(var = "CTD_END_TIME", des = "The time at the end of the profiling station") %>%  
  add_row(var = "CTD_END_LATITUDE", des = "The latitude at the end of the profiling station") %>%  
  add_row(var = "CTD_END_LONGITUDE", des = "The longitude at the end of the profiling station") %>% 
  add_row(var = "missing_value", des = "The value that corresponds to missing data within the data table") %>% 

  # Ocean data variables
  add_row(var = "CTDPRS", des = "Pressure") %>%
  add_row(var = "CTDTMP", des = "Temperature") %>%
  add_row(var = "CTDSAL", des = "Salinity") %>%
  add_row(var = "CTDDOXY", des = "Dissolved oxygen") %>%
  add_row(var = "CTDFLUOR", des = "Fluorescence") %>%
  add_row(var = "CTDBEAMCP", des = "Beam attenuation") %>%
  add_row(var = "CTDBBP700", des = "Optical backscatter at 700 nm") %>%
  add_row(var = "CTDXMISS", des = "Transmissometer") %>%
  add_row(var = "CTDPAR", des = "Photosynthetically active radiation") %>%
  add_row(var = "CTDNITRATE", des ="Nitrate")
    
colnames(prof_rec) = c("Variable", "Description")

prof_rec_adj = rbind(rep("Header information", 2), prof_rec[1:7,], rep("Header variables",2), prof_rec[8:24,], rep("Ocean data variables", 2), prof_rec[25:35,])

prof_rec_adj %>% flextable() %>% autofit() %>% merge_h() %>% bold(i = c(1, 9, 27)) %>% set_table_properties(layout = "autofit")

#knitr::kable(prof_rec) %>% kable_paper("striped", full_width = F) %>% pack_rows("Header information", 1,7)  %>% pack_rows("Header variables", 8,24) %>% pack_rows("Ocean data variables", 25, 35) 

```

```{r pigrecords, tab.cap = "a figure", echo = FALSE, message = FALSE, warning=FALSE}
pig_rec = data.frame("var" = as.character(), "des" = as.character())
# Header information
pig_rec = pig_rec %>% 
  add_row(var = "ORIGINAL_CHLFILE/S", des = "The name of the original file/s") %>%
  add_row(var = "CHLFILE_MOD_DATE", des = "The modification date of the file") %>%
  add_row(var = "SOURCED_FROM", des = "The repository where data was originally sourced from") %>%
  add_row(var = "ANALYSIS_METHOD", des = "The analysis method used to obtain data") %>% 
  add_row(var = "DATASET_CONTACT", des = "Name and email of the listed dataset contact") %>%
  add_row(var ="DOI/s", des = "Doi/s of original files") %>%
  add_row(var = "BIOMATE_CITE_TAGS", des = "The BIOMATE citation tags that are associated with the data, methods and source repository") %>%
  add_row(var = "DATA_CITATION/S", des = "The full citations associted with the data" ) %>%
  add_row(var = "METHOD_CITATION/S", des = "The full citation associated with the method used to analyse the water sample for pigments") %>%
  
# Header variables
  add_row(var = "NUMBER_HEADERS", des = "The number of header variables") %>%
  add_row(var = "EXPOCODE", des = "The EXPOCODE associated with the data" ) %>% 
  add_row(var = "SHIP", des = "The vessel on which the data was collected") %>%
  add_row(var = "TIMEZONE", des = "The timezone the data was collected in") %>%
  add_row(var = "missing_value", des = "The value that corresponds to missing data within the data table") %>% 
  add_row(var = "not_detected", des = "The value that corresponds to data not detected in analysis within the data table") %>%

  # Ocean data variables
  add_row(var = "CTD_IDs", des = "An identifcation for a matching profiling station in the profiling sensor stream") %>% 
  add_row(var = "DATE", des = "The date of the profiling station") %>% 
  add_row(var = "TIME_s", des = "The start time of the profiling station") %>%
  add_row(var = "TIME_b", des = "The bottom time of the profiling start date") %>% 
  add_row(var = "TIME_e", des = "The end time of the profiling station") %>%
  add_row(var = "LATITUDE", des = "The start latitude of the profiling station") %>%
  add_row(var = "LONGITUDE", des = "The start longitude of the profiling station") %>%
  add_row(var = "STNNBR", des = "The station number of the profiling station") %>% 
  add_row(var = "CASTNO", des = "The cast number of the profiling station") %>% 
  add_row(var = "DATE_analyser", des = "The date of sampling as recorded by the analyser") %>%
  add_row(var = "TIME_analyser", des = "The time of samping as recorded by the analyser") %>%
  add_row(var = "LAT_analyser", des = "The latitude at sampling as recorded by the analyser") %>%
  add_row(var = "LON_analyser", des = "The longitude at sampling as recorded by the analyser") %>%
  add_row(var = "STNNBR_analyser", des = "The station number of the profiling station as recorded by the analyser") %>%
  add_row(var = "CASTNO_analyser", des = "The cast number of the profiling station as recorded by the analyser") %>%
  add_row(var = "Sample_ID", des = "The sample identification as recorded by the analyser") %>%
  add_row(var = "BOTTLE", des = "The rosette bottle number as recorded by the analyser") %>%
  add_row(var = "DEPTH", des = "The depth the sample was taken") %>% 
  
  add_row(var = "FCHLORA", des = "Fluorometrically derived chlorophyll") %>%
  add_row(var = "FPHEO", des = "fluorometrically derived phaeopigments") %>%
  add_row(var = "FPHYTIN", des = "fluorometrically derived phaeophytin") %>% 
  add_row(var = "TCHLA", des = "HPLC derived total chlorophyll a") %>%
  add_row(var = "TACC", des = "HPLC derived total accessory pigments") %>%
  add_row(var = "DVChla", des = "HPLC derived divinyl chlorophyll a") %>% 
  add_row(var = "Chla", des = "HPLC derived chlorophyll a") %>%
  add_row(var = "Chla_ide", des = "HPLC derived chlorophyllide") %>% 
  add_row(var = "Chla_allom", des = "HPLC derived chlorophyll a allomers") %>%
  add_row(var = "Chla_prime", des = "PLC derived chlorophyll a prime") %>%
  add_row(var = "Chlb", des = "HPLC derived chlorophyll b") %>%
  add_row(var = "DVChlb", des = "HPLC derived divinyl chlorophyll b") %>%
  add_row(var = "Chlc", des = "HPLC derived chlorophyll c") %>%
  add_row(var = "Chlc1_Chlc2_Mg_3_8_divinyl_pheoporphyrin_a5", des = "HPLC derived chlorophyll c1 + chlorophyll c2 + Mg 3,8 divinyl pheoporphyrin a5") %>%
  add_row(var = "Chlc1", des ="HPLC derived chlorophyll c1" ) %>%
  add_row(var = "Chlc1_like", des = "HPLC derived chlorophyll c1-like") %>%
  add_row(var = "Chlc2", des = "HPLC derived chlorophyll c2") %>%
  add_row(var = "Chlc1_Chlc2", des = "HPLC derived chlorophyll c1 + chlorophyll c2") %>%
  add_row(var = "Chlc3", des = "HPLC derived chlorophyll c3") %>%
  add_row(var = "MgDVP", des = "HPLC derived Mg 2,4 divinyl pheoporphyrin a5 monomethyl ester") %>%
  add_row(var = "19Hex", des = "HPLC derived 19’hexanoyloxyfucoxanthin") %>%
  add_row(var = "19But", des = "HPLC derived 19’butanoyloxyfucoxanthin") %>%
  add_row(var = "Fucox", des = "HPLC derived fucoxanthin") %>%
  add_row(var = "Prasino", des = "HPLC derived alloxanthin") %>%
  add_row(var = "Lutein", des = "HPLC derived lutein") %>%
  add_row(var = "Zeax", des = "HPLC derived zeaxanthin") %>%
  add_row(var = "Zea_Lut", des = "HPLC derived zeaxanthin + lutein") %>%
  add_row(var = "Violax", des = "HPLC derived violaxanthin") %>%
  add_row(var = "Alpha_car", des = "HPLC derived alpha carotene") %>%
  add_row(var = "Beta_car", des = "HPLC derived beta carotene") %>%
  add_row(var ="Gamma_car", des = "HPLC derived gamma carotene") %>%
  add_row(var = "Epsilon_car", des = "HPLC derived epsilon carotene") %>%
  add_row(var = "Alpha_Beta_car", des = "HPLC derived alpha + beta carotene") %>%
  add_row(var = "Neox", des = "HPLC derived neoxanthin") %>%
  add_row(var = "DD", des = "HPLC derived diadinoxanthin") %>%
  add_row(var = "DT", des = "HPLC derived diatoxanthin") %>%
  add_row(var = "Viol_Neox", des = "HPLC derived violaxanthin + neoxanthin") %>%
  add_row(var = "Phaeopigments", des = "HPLC derived bulk phaeopigments") %>%
  add_row(var = "Phide_a", des = "HPLC derived phaeophorbide a") %>%
  add_row(var = "Phytin_a", des = "HPLC derived phaeophytin a")

  
colnames(pig_rec) = c("Variable", "Description")
  
pig_rec_adj = rbind(rep("Header information", 2), pig_rec[1:9,], rep("Header variables",2), pig_rec[10:15,], rep("Ocean data variables", 2), pig_rec[16:73,])

pig_rec_adj %>% flextable() %>% autofit() %>% merge_h() %>% bold(i = c(1, 11,18)) %>% set_table_properties(layout = "autofit")


#knitr::kable(pig_rec) %>% kable_paper("striped", full_width = F) %>% pack_rows("Header information", 1,9)  %>% pack_rows("Header variables", 10,15) %>% pack_rows("Ocean data variables", 16, 73) 
```



```{r uwyrecords, tab.cap = "a figure", echo = FALSE, message = FALSE, warning=FALSE}

```

```{r pocrecords, tab.cap = "a figure", echo = FALSE, message = FALSE, warning=FALSE}

```





# References
